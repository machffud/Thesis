{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6466ac35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f26aae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  3 09:35:37 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.07             Driver Version: 535.161.07   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          On  | 00000000:07:00.0 Off |                    0 |\n",
      "| N/A   46C    P0             246W / 400W |  12126MiB / 40960MiB |     65%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100-SXM4-40GB          On  | 00000000:0F:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              53W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100-SXM4-40GB          On  | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              53W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100-SXM4-40GB          On  | 00000000:4E:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              52W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100-SXM4-40GB          On  | 00000000:87:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              53W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100-SXM4-40GB          On  | 00000000:90:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              58W / 400W |  29258MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100-SXM4-40GB          On  | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   29C    P0              53W / 400W |      3MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100-SXM4-40GB          On  | 00000000:BD:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              62W / 400W |  17216MiB / 40960MiB |     32%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3988aa4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.8/site-packages (4.39.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers) (3.3.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (1.11.0a0+b6df043)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch) (4.10.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9b034e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.8/site-packages (8.1.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (7.29.0)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.8/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (2.10.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.0)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (58.5.3)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.22)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.8/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.8/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08379bc",
   "metadata": {},
   "source": [
    "### if this is your first time installing the above library, you must restart your kernel and rerun before continuing to the next code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88f7d5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27acae09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Tugas Akhir/indonlu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%cd indonlu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9dd923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "# %cd utils  \n",
    "from utils.forward_fn import forward_sequence_classification\n",
    "from utils.metrics import document_sentiment_metrics_fn\n",
    "from utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b764f9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c2d4949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Tugas Akhir\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b972d",
   "metadata": {},
   "source": [
    "### Since this research has an experiment to know the effective representation of text\n",
    "### and all of representation already precompute, you can choose 1 of below list for an input representation\n",
    "### `[\"title_ori\", \"text_ori\", \"tc_ori\", \"filter_word_tc\", \"trunc_first_short\", \"trunc_last_short\", \"trunc_first_long\", \"trunc_last_long\", \"trunc_combination\",  \"textrank\", \"sum_indobart\", \"sum_liputan6\",\"sum_pegasus\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79597366",
   "metadata": {},
   "outputs": [],
   "source": [
    "representasi_teks = 'title_ori'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a23365e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####\n",
    "# Document Sentiment Prosa\n",
    "#####\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class DocumentBencanaDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = {'Y': 1, 'N': 0}\n",
    "    INDEX2LABEL = {0: 'N', 1: 'Y'}\n",
    "    NUM_LABELS = 2\n",
    "    \n",
    "    def load_dataset(self, path): \n",
    "        aa  = pd.read_csv(path, sep=',')\n",
    "        df = aa[[representasi_teks,'label']]\n",
    "        df.columns = [representasi_teks,'label']\n",
    "        df['label'] = df['label'].apply(lambda lab: self.LABEL2INDEX[lab])\n",
    "        return df\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer, no_special_token=False, *args, **kwargs):\n",
    "        self.data = self.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.no_special_token = no_special_token\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data.loc[index,:]\n",
    "        text, sentiment = data[representasi_teks], data['label']\n",
    "        subwords = self.tokenizer.encode(text, add_special_tokens=not self.no_special_token)\n",
    "        return np.array(subwords), np.array(sentiment), data[representasi_teks]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)    \n",
    "        \n",
    "class DocumentBencanaDataLoader(DataLoader):\n",
    "    def __init__(self, max_seq_len=256, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        \n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
    "        sentiment_batch = np.zeros((batch_size, 1), dtype=np.int64)\n",
    "        \n",
    "        seq_list = []\n",
    "        for i, (subwords, sentiment, raw_seq) in enumerate(batch):\n",
    "            subwords = subwords[:max_seq_len]\n",
    "            subword_batch[i,:len(subwords)] = subwords\n",
    "            mask_batch[i,:len(subwords)] = 1\n",
    "            sentiment_batch[i,0] = sentiment\n",
    "            \n",
    "            seq_list.append(raw_seq)\n",
    "            \n",
    "        return subword_batch, mask_batch, sentiment_batch, seq_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d34850d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# common functions\n",
    "###\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "932899cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_seed(28022023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fa52960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-large-p2')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-large-p2', hidden_size = 128)\n",
    "config.num_labels = DocumentBencanaDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-large-p2', config=config,ignore_mismatched_sizes=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0bc003e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83e03427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Tugas Akhir/dataset\n"
     ]
    }
   ],
   "source": [
    "%cd dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67114a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval(df, bencana):\n",
    "    act = pd.read_csv(f'./{bencana}/{bencana}_test.csv')\n",
    "    yact = list(act['label'])\n",
    "    ypred = list(df['label'])\n",
    "    p,r, f,s = precision_recall_fscore_support(yact,ypred, average='binary', pos_label='Y')\n",
    "    print('Evaluasi bencana', bencana)\n",
    "    print(f\"Nilai Precision = {p}\")\n",
    "    print(f\"Nilai Recall = {r}\")\n",
    "    print(f\"Nilai F1 = {f}\")\n",
    "    return p,r,f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c58f281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(n_epochs, train_loader, valid_loader,optimizer, model, i2w, bencana,test_loader , nomor_run):\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "    \n",
    "        total_train_loss = 0\n",
    "        list_hyp, list_label = [], []\n",
    "\n",
    "        train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
    "        for i, batch_data in enumerate(train_pbar):\n",
    "            # Forward model\n",
    "            loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "            # Update model\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss = loss.item()\n",
    "            total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "            # Calculate metrics\n",
    "            list_hyp += batch_hyp\n",
    "            list_label += batch_label\n",
    "\n",
    "            train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "                total_train_loss/(i+1), get_lr(optimizer)))\n",
    "\n",
    "        # Calculate train metric\n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "        print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "\n",
    "        # Evaluate on validation\n",
    "        model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "        \n",
    "        total_loss, total_correct, total_labels = 0, 0, 0\n",
    "        list_hyp, list_label = [], []\n",
    "\n",
    "        pbar = tqdm(valid_loader, leave=True, total=len(valid_loader))\n",
    "        for i, batch_data in enumerate(pbar):\n",
    "            batch_seq = batch_data[-1]        \n",
    "            loss, batch_hyp, batch_label = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "            \n",
    "            # Calculate total loss\n",
    "            valid_loss = loss.item()\n",
    "            total_loss = total_loss + valid_loss\n",
    "\n",
    "            # Calculate evaluation metrics\n",
    "            list_hyp += batch_hyp\n",
    "            list_label += batch_label\n",
    "            metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "\n",
    "            pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "            \n",
    "        metrics = document_sentiment_metrics_fn(list_hyp, list_label)\n",
    "        print(\"(Epoch {}) VALID LOSS:{:.4f} {}\".format((epoch+1),\n",
    "            total_loss/(i+1), metrics_to_string(metrics)))\n",
    "    model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "    list_hyp, list_label = [], []\n",
    "\n",
    "    pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        _, batch_hyp, _ = forward_sequence_classification(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "        list_hyp += batch_hyp\n",
    "\n",
    "    # Save prediction result_classification_Indobert\n",
    "    df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
    "    df['id'] = list(pd.read_csv(f'./{bencana}/{bencana}_test.csv')['id'])\n",
    "    df[['id','label']].to_csv(f'../prediction/result_classification_Indobert/{bencana}_{representasi_teks}_{nomor_run+1}.csv', index=False)\n",
    "    torch.cuda.empty_cache()\n",
    "    return get_eval(df, bencana)\n",
    "    \n",
    "\n",
    "def run_bert_for_bencana(bencana, nomor_run):\n",
    "    tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-large-p2')\n",
    "    config = BertConfig.from_pretrained('indobenchmark/indobert-large-p2', hidden_size = 768)\n",
    "    config.num_labels = DocumentBencanaDataset.NUM_LABELS\n",
    "\n",
    "    # Instantiate model\n",
    "    model_run = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-large-p2', config=config,ignore_mismatched_sizes=True)\n",
    "\n",
    "    #model_run = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-large-p2', config=config,ignore_mismatched_sizes=True)\n",
    "    train_dataset_path = f'./{bencana}/{bencana}_train.csv'\n",
    "    valid_dataset_path = f'./{bencana}/{bencana}_dev.csv'\n",
    "    test_dataset_path = f'./{bencana}/{bencana}_test.csv'\n",
    "    train_dataset = DocumentBencanaDataset(train_dataset_path, tokenizer, lowercase=True)\n",
    "    valid_dataset = DocumentBencanaDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
    "    test_dataset = DocumentBencanaDataset(test_dataset_path, tokenizer, lowercase=True)\n",
    "\n",
    "    train_loader = DocumentBencanaDataLoader(dataset=train_dataset, max_seq_len=256, batch_size=24, num_workers=16, shuffle=True)  \n",
    "    valid_loader = DocumentBencanaDataLoader(dataset=valid_dataset, max_seq_len=256, batch_size=24, num_workers=16, shuffle=False)  \n",
    "    test_loader = DocumentBencanaDataLoader(dataset=test_dataset, max_seq_len=256, batch_size=24, num_workers=16, shuffle=False)\n",
    "    w2i, i2w = DocumentBencanaDataset.LABEL2INDEX, DocumentBencanaDataset.INDEX2LABEL\n",
    "    optimizer = optim.Adam(model_run.parameters(), lr=3e-7)\n",
    "    model_run = model_run.cuda()\n",
    "    return train_bert(24, train_loader, valid_loader,optimizer, model_run, i2w, bencana, test_loader, nomor_run )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2edc5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_score_format(fl,pl,rl):\n",
    "    print(f\"Nilai maksimal dari semua run:\")\n",
    "    print(\"P =\", max(pl))\n",
    "    print(\"R =\", max(rl))\n",
    "    print(\"F1 =\", max(fl))\n",
    "    \n",
    "    print(f\"Nilai minimal dari semua run:\")\n",
    "    print(\"P =\", min(pl))\n",
    "    print(\"R =\", min(rl))\n",
    "    print(\"F1 =\", min(fl))\n",
    "    \n",
    "    print(f\"Nilai rata-rata dari semua run:\")\n",
    "    print(\"P =\", sum(pl)/len(pl))\n",
    "    print(\"R =\", sum(rl)/len(rl))\n",
    "    print(\"F1 =\", sum(fl)/len(fl))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb242e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ke 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 768]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 768]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.4784 LR:0.00000100: 100%|| 38/38 [00:03<00:00,  9.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.4784 ACC:0.82 F1:0.01 REC:0.01 PRE:0.14 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6719 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.6719 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4695 LR:0.00000100: 100%|| 38/38 [00:03<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4695 ACC:0.83 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7196 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.7196 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4520 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4520 ACC:0.83 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6329 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6329 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4352 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4352 ACC:0.83 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6867 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6867 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.3926 LR:0.00000100: 100%|| 38/38 [00:03<00:00,  9.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.3926 ACC:0.83 F1:0.10 REC:0.05 PRE:1.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4597 ACC:0.84 F1:0.72 REC:0.66 PRE:0.81: 100%|| 5/5 [00:01<00:00,  4.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.4597 ACC:0.84 F1:0.72 REC:0.66 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3427 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3427 ACC:0.85 F1:0.33 REC:0.21 PRE:0.85 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3318 ACC:0.85 F1:0.74 REC:0.66 PRE:0.84: 100%|| 5/5 [00:01<00:00,  4.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.3318 ACC:0.85 F1:0.74 REC:0.66 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.2808 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.2808 ACC:0.89 F1:0.61 REC:0.47 PRE:0.86 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3621 ACC:0.86 F1:0.73 REC:0.59 PRE:0.95: 100%|| 5/5 [00:01<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.3621 ACC:0.86 F1:0.73 REC:0.59 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.2600 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.2600 ACC:0.90 F1:0.66 REC:0.57 PRE:0.78 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3135 ACC:0.84 F1:0.72 REC:0.66 PRE:0.81: 100%|| 5/5 [00:01<00:00,  3.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.3135 ACC:0.84 F1:0.72 REC:0.66 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.2312 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.2312 ACC:0.91 F1:0.71 REC:0.64 PRE:0.81 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3080 ACC:0.86 F1:0.76 REC:0.69 PRE:0.85: 100%|| 5/5 [00:01<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.3080 ACC:0.86 F1:0.76 REC:0.69 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.2098 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.2098 ACC:0.92 F1:0.75 REC:0.65 PRE:0.87 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3139 ACC:0.85 F1:0.74 REC:0.66 PRE:0.84: 100%|| 5/5 [00:01<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.3139 ACC:0.85 F1:0.74 REC:0.66 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.2070 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.2070 ACC:0.92 F1:0.75 REC:0.68 PRE:0.84 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3234 ACC:0.85 F1:0.74 REC:0.66 PRE:0.84: 100%|| 5/5 [00:01<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.3234 ACC:0.85 F1:0.74 REC:0.66 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.2014 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.2014 ACC:0.92 F1:0.77 REC:0.72 PRE:0.83 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4533 ACC:0.86 F1:0.73 REC:0.59 PRE:0.95: 100%|| 5/5 [00:01<00:00,  4.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.4533 ACC:0.86 F1:0.73 REC:0.59 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.1641 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.1641 ACC:0.94 F1:0.81 REC:0.78 PRE:0.84 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3502 ACC:0.85 F1:0.74 REC:0.66 PRE:0.84: 100%|| 5/5 [00:01<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.3502 ACC:0.85 F1:0.74 REC:0.66 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.1764 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.1764 ACC:0.93 F1:0.80 REC:0.75 PRE:0.85 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3523 ACC:0.86 F1:0.77 REC:0.75 PRE:0.80: 100%|| 5/5 [00:01<00:00,  3.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.3523 ACC:0.86 F1:0.77 REC:0.75 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.1589 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.1589 ACC:0.94 F1:0.83 REC:0.80 PRE:0.86 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3754 ACC:0.86 F1:0.79 REC:0.81 PRE:0.76: 100%|| 5/5 [00:01<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.3754 ACC:0.86 F1:0.79 REC:0.81 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.1633 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.1633 ACC:0.94 F1:0.82 REC:0.81 PRE:0.84 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4037 ACC:0.85 F1:0.73 REC:0.62 PRE:0.87: 100%|| 5/5 [00:01<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.4037 ACC:0.85 F1:0.73 REC:0.62 PRE:0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.1645 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.1645 ACC:0.93 F1:0.81 REC:0.79 PRE:0.82 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3738 ACC:0.85 F1:0.76 REC:0.75 PRE:0.77: 100%|| 5/5 [00:01<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.3738 ACC:0.85 F1:0.76 REC:0.75 PRE:0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.1378 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.1378 ACC:0.95 F1:0.86 REC:0.84 PRE:0.89 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3607 ACC:0.85 F1:0.75 REC:0.72 PRE:0.79: 100%|| 5/5 [00:01<00:00,  3.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.3607 ACC:0.85 F1:0.75 REC:0.72 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.1408 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.1408 ACC:0.95 F1:0.84 REC:0.84 PRE:0.84 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6584 ACC:0.81 F1:0.58 REC:0.41 PRE:1.00: 100%|| 5/5 [00:01<00:00,  3.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.6584 ACC:0.81 F1:0.58 REC:0.41 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.1538 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.1538 ACC:0.94 F1:0.83 REC:0.79 PRE:0.88 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4305 ACC:0.84 F1:0.70 REC:0.59 PRE:0.86: 100%|| 5/5 [00:01<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.4305 ACC:0.84 F1:0.70 REC:0.59 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.1131 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.1131 ACC:0.96 F1:0.88 REC:0.87 PRE:0.89 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3849 ACC:0.87 F1:0.77 REC:0.69 PRE:0.88: 100%|| 5/5 [00:01<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.3849 ACC:0.87 F1:0.77 REC:0.69 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.0961 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.0961 ACC:0.97 F1:0.91 REC:0.89 PRE:0.92 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3783 ACC:0.86 F1:0.77 REC:0.75 PRE:0.80: 100%|| 5/5 [00:01<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.3783 ACC:0.86 F1:0.77 REC:0.75 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.1042 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.1042 ACC:0.95 F1:0.87 REC:0.86 PRE:0.88 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4060 ACC:0.87 F1:0.77 REC:0.69 PRE:0.88: 100%|| 5/5 [00:01<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.4060 ACC:0.87 F1:0.77 REC:0.69 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.0898 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.0898 ACC:0.96 F1:0.90 REC:0.88 PRE:0.92 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5133 ACC:0.85 F1:0.72 REC:0.59 PRE:0.90: 100%|| 5/5 [00:01<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5133 ACC:0.85 F1:0.72 REC:0.59 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:01<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana angin_topan\n",
      "Nilai Precision = 0.8787878787878788\n",
      "Nilai Recall = 0.6304347826086957\n",
      "Nilai F1 = 0.7341772151898734\n",
      "run ke 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 768]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 768]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.4932 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.4932 ACC:0.82 F1:0.01 REC:0.01 PRE:0.14 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6602 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.6602 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4625 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4625 ACC:0.83 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6472 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  3.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.6472 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4446 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4446 ACC:0.83 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5649 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.5649 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4300 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4300 ACC:0.83 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5136 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.5136 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4061 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4061 ACC:0.83 F1:0.01 REC:0.01 PRE:1.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4168 ACC:0.81 F1:0.60 REC:0.44 PRE:0.93: 100%|| 5/5 [00:01<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.4168 ACC:0.81 F1:0.60 REC:0.44 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3357 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3357 ACC:0.86 F1:0.33 REC:0.20 PRE:0.91 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3623 ACC:0.81 F1:0.60 REC:0.44 PRE:0.93: 100%|| 5/5 [00:01<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.3623 ACC:0.81 F1:0.60 REC:0.44 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.2822 LR:0.00000100: 100%|| 38/38 [00:03<00:00,  9.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.2822 ACC:0.89 F1:0.57 REC:0.43 PRE:0.83 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4459 ACC:0.81 F1:0.60 REC:0.44 PRE:0.93: 100%|| 5/5 [00:01<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.4459 ACC:0.81 F1:0.60 REC:0.44 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.2396 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.2396 ACC:0.91 F1:0.73 REC:0.65 PRE:0.82 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5766 ACC:0.79 F1:0.51 REC:0.34 PRE:1.00: 100%|| 5/5 [00:01<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.5766 ACC:0.79 F1:0.51 REC:0.34 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.2358 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.2358 ACC:0.92 F1:0.73 REC:0.64 PRE:0.86 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3627 ACC:0.84 F1:0.69 REC:0.56 PRE:0.90: 100%|| 5/5 [00:01<00:00,  3.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.3627 ACC:0.84 F1:0.69 REC:0.56 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.2169 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.2169 ACC:0.92 F1:0.74 REC:0.70 PRE:0.80 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6296 ACC:0.78 F1:0.48 REC:0.31 PRE:1.00: 100%|| 5/5 [00:01<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.6296 ACC:0.78 F1:0.48 REC:0.31 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.2139 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.2139 ACC:0.92 F1:0.76 REC:0.70 PRE:0.83 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3298 ACC:0.84 F1:0.69 REC:0.56 PRE:0.90: 100%|| 5/5 [00:01<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.3298 ACC:0.84 F1:0.69 REC:0.56 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.1790 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.1790 ACC:0.94 F1:0.81 REC:0.77 PRE:0.85 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5122 ACC:0.82 F1:0.62 REC:0.47 PRE:0.94: 100%|| 5/5 [00:01<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5122 ACC:0.82 F1:0.62 REC:0.47 PRE:0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.1683 LR:0.00000100: 100%|| 38/38 [00:03<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.1683 ACC:0.94 F1:0.81 REC:0.77 PRE:0.87 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2889 ACC:0.86 F1:0.75 REC:0.66 PRE:0.88: 100%|| 5/5 [00:01<00:00,  3.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.2889 ACC:0.86 F1:0.75 REC:0.66 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.1613 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.1613 ACC:0.94 F1:0.83 REC:0.80 PRE:0.86 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3165 ACC:0.86 F1:0.75 REC:0.66 PRE:0.88: 100%|| 5/5 [00:01<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.3165 ACC:0.86 F1:0.75 REC:0.66 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.1614 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.1614 ACC:0.94 F1:0.82 REC:0.80 PRE:0.85 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4263 ACC:0.83 F1:0.67 REC:0.53 PRE:0.89: 100%|| 5/5 [00:01<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.4263 ACC:0.83 F1:0.67 REC:0.53 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.1282 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.1282 ACC:0.95 F1:0.87 REC:0.83 PRE:0.90 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4347 ACC:0.82 F1:0.65 REC:0.53 PRE:0.85: 100%|| 5/5 [00:01<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.4347 ACC:0.82 F1:0.65 REC:0.53 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.1582 LR:0.00000100: 100%|| 38/38 [00:03<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.1582 ACC:0.95 F1:0.84 REC:0.81 PRE:0.88 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2898 ACC:0.88 F1:0.82 REC:0.84 PRE:0.79: 100%|| 5/5 [00:01<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.2898 ACC:0.88 F1:0.82 REC:0.84 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.1380 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.1380 ACC:0.95 F1:0.87 REC:0.86 PRE:0.88 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5144 ACC:0.82 F1:0.64 REC:0.50 PRE:0.89: 100%|| 5/5 [00:01<00:00,  3.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.5144 ACC:0.82 F1:0.64 REC:0.50 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.1263 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.1263 ACC:0.95 F1:0.87 REC:0.85 PRE:0.89 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3146 ACC:0.88 F1:0.79 REC:0.72 PRE:0.88: 100%|| 5/5 [00:01<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.3146 ACC:0.88 F1:0.79 REC:0.72 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.1204 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.1204 ACC:0.96 F1:0.87 REC:0.84 PRE:0.91 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3789 ACC:0.85 F1:0.73 REC:0.62 PRE:0.87: 100%|| 5/5 [00:01<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.3789 ACC:0.85 F1:0.73 REC:0.62 PRE:0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.1163 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.1163 ACC:0.95 F1:0.87 REC:0.86 PRE:0.88 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3490 ACC:0.85 F1:0.73 REC:0.62 PRE:0.87: 100%|| 5/5 [00:01<00:00,  3.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.3490 ACC:0.85 F1:0.73 REC:0.62 PRE:0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.1169 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.1169 ACC:0.96 F1:0.88 REC:0.87 PRE:0.89 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5259 ACC:0.83 F1:0.67 REC:0.53 PRE:0.89: 100%|| 5/5 [00:01<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.5259 ACC:0.83 F1:0.67 REC:0.53 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.1093 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.1093 ACC:0.96 F1:0.89 REC:0.87 PRE:0.91 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4521 ACC:0.85 F1:0.73 REC:0.62 PRE:0.87: 100%|| 5/5 [00:01<00:00,  3.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.4521 ACC:0.85 F1:0.73 REC:0.62 PRE:0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.1073 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.1073 ACC:0.96 F1:0.87 REC:0.86 PRE:0.89 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5845 ACC:0.82 F1:0.64 REC:0.50 PRE:0.89: 100%|| 5/5 [00:01<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5845 ACC:0.82 F1:0.64 REC:0.50 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:01<00:00,  7.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana angin_topan\n",
      "Nilai Precision = 0.84\n",
      "Nilai Recall = 0.45652173913043476\n",
      "Nilai F1 = 0.5915492957746479\n",
      "run ke 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 768]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 768]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 768]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([768, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([768, 768]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.4931 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.4931 ACC:0.81 F1:0.07 REC:0.04 PRE:0.23 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6757 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  3.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.6757 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4605 LR:0.00000100: 100%|| 38/38 [00:03<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4605 ACC:0.83 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6700 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  3.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.6700 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4517 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4517 ACC:0.83 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6484 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:01<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6484 ACC:0.68 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4289 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4289 ACC:0.83 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5114 ACC:0.70 F1:0.12 REC:0.06 PRE:1.00: 100%|| 5/5 [00:01<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.5114 ACC:0.70 F1:0.12 REC:0.06 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.3985 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.3985 ACC:0.83 F1:0.05 REC:0.03 PRE:0.80 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4612 ACC:0.72 F1:0.22 REC:0.12 PRE:1.00: 100%|| 5/5 [00:01<00:00,  3.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.4612 ACC:0.72 F1:0.22 REC:0.12 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3397 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3397 ACC:0.87 F1:0.45 REC:0.30 PRE:0.92 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3428 ACC:0.85 F1:0.72 REC:0.59 PRE:0.90: 100%|| 5/5 [00:01<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.3428 ACC:0.85 F1:0.72 REC:0.59 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.2837 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.2837 ACC:0.90 F1:0.62 REC:0.48 PRE:0.87 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3414 ACC:0.84 F1:0.69 REC:0.56 PRE:0.90: 100%|| 5/5 [00:01<00:00,  3.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.3414 ACC:0.84 F1:0.69 REC:0.56 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.2542 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.2542 ACC:0.91 F1:0.69 REC:0.58 PRE:0.84 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3016 ACC:0.91 F1:0.86 REC:0.84 PRE:0.87: 100%|| 5/5 [00:01<00:00,  4.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.3016 ACC:0.91 F1:0.86 REC:0.84 PRE:0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.2567 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.2567 ACC:0.91 F1:0.69 REC:0.58 PRE:0.85 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4357 ACC:0.82 F1:0.62 REC:0.47 PRE:0.94: 100%|| 5/5 [00:01<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.4357 ACC:0.82 F1:0.62 REC:0.47 PRE:0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.2233 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.2233 ACC:0.92 F1:0.75 REC:0.68 PRE:0.83 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2833 ACC:0.88 F1:0.79 REC:0.69 PRE:0.92: 100%|| 5/5 [00:01<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.2833 ACC:0.88 F1:0.79 REC:0.69 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.2169 LR:0.00000100: 100%|| 38/38 [00:03<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.2169 ACC:0.92 F1:0.73 REC:0.65 PRE:0.83 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4972 ACC:0.79 F1:0.53 REC:0.38 PRE:0.92: 100%|| 5/5 [00:01<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.4972 ACC:0.79 F1:0.53 REC:0.38 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.1999 LR:0.00000100: 100%|| 38/38 [00:03<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.1999 ACC:0.94 F1:0.80 REC:0.72 PRE:0.89 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3640 ACC:0.88 F1:0.82 REC:0.88 PRE:0.78: 100%|| 5/5 [00:01<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.3640 ACC:0.88 F1:0.82 REC:0.88 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.2054 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.2054 ACC:0.94 F1:0.80 REC:0.73 PRE:0.88 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2809 ACC:0.90 F1:0.83 REC:0.75 PRE:0.92: 100%|| 5/5 [00:01<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.2809 ACC:0.90 F1:0.83 REC:0.75 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.1805 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.1805 ACC:0.93 F1:0.79 REC:0.74 PRE:0.84 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3155 ACC:0.88 F1:0.79 REC:0.69 PRE:0.92: 100%|| 5/5 [00:01<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.3155 ACC:0.88 F1:0.79 REC:0.69 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.1780 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.1780 ACC:0.94 F1:0.81 REC:0.75 PRE:0.89 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2702 ACC:0.91 F1:0.85 REC:0.78 PRE:0.93: 100%|| 5/5 [00:01<00:00,  3.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.2702 ACC:0.91 F1:0.85 REC:0.78 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.1724 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.1724 ACC:0.94 F1:0.80 REC:0.74 PRE:0.87 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3286 ACC:0.88 F1:0.82 REC:0.88 PRE:0.78: 100%|| 5/5 [00:01<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.3286 ACC:0.88 F1:0.82 REC:0.88 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.1593 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.1593 ACC:0.94 F1:0.82 REC:0.79 PRE:0.86 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3224 ACC:0.88 F1:0.79 REC:0.69 PRE:0.92: 100%|| 5/5 [00:01<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.3224 ACC:0.88 F1:0.79 REC:0.69 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.1879 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.1879 ACC:0.93 F1:0.78 REC:0.72 PRE:0.84 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3979 ACC:0.85 F1:0.79 REC:0.88 PRE:0.72: 100%|| 5/5 [00:01<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.3979 ACC:0.85 F1:0.79 REC:0.88 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.1566 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.1566 ACC:0.95 F1:0.83 REC:0.77 PRE:0.90 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2688 ACC:0.91 F1:0.85 REC:0.78 PRE:0.93: 100%|| 5/5 [00:01<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.2688 ACC:0.91 F1:0.85 REC:0.78 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.1364 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.1364 ACC:0.95 F1:0.86 REC:0.82 PRE:0.92 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3622 ACC:0.86 F1:0.80 REC:0.88 PRE:0.74: 100%|| 5/5 [00:01<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.3622 ACC:0.86 F1:0.80 REC:0.88 PRE:0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.1135 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.1135 ACC:0.96 F1:0.87 REC:0.84 PRE:0.90 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2847 ACC:0.93 F1:0.89 REC:0.88 PRE:0.90: 100%|| 5/5 [00:01<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.2847 ACC:0.93 F1:0.89 REC:0.88 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.1177 LR:0.00000100: 100%|| 38/38 [00:05<00:00,  7.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.1177 ACC:0.96 F1:0.88 REC:0.86 PRE:0.90 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3183 ACC:0.89 F1:0.81 REC:0.72 PRE:0.92: 100%|| 5/5 [00:01<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.3183 ACC:0.89 F1:0.81 REC:0.72 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.1217 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  8.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.1217 ACC:0.95 F1:0.87 REC:0.84 PRE:0.90 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3109 ACC:0.91 F1:0.86 REC:0.88 PRE:0.85: 100%|| 5/5 [00:01<00:00,  3.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.3109 ACC:0.91 F1:0.86 REC:0.88 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.1211 LR:0.00000100: 100%|| 38/38 [00:04<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.1211 ACC:0.95 F1:0.87 REC:0.86 PRE:0.88 LR:0.00000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3034 ACC:0.90 F1:0.83 REC:0.75 PRE:0.92: 100%|| 5/5 [00:01<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.3034 ACC:0.90 F1:0.83 REC:0.75 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:01<00:00,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana angin_topan\n",
      "Nilai Precision = 0.9\n",
      "Nilai Recall = 0.782608695652174\n",
      "Nilai F1 = 0.8372093023255814\n",
      "Nilai maksimal dari semua run:\n",
      "P = 0.9\n",
      "R = 0.782608695652174\n",
      "F1 = 0.8372093023255814\n",
      "Nilai minimal dari semua run:\n",
      "P = 0.84\n",
      "R = 0.45652173913043476\n",
      "F1 = 0.5915492957746479\n",
      "Nilai rata-rata dari semua run:\n",
      "P = 0.8729292929292929\n",
      "R = 0.6231884057971014\n",
      "F1 = 0.7209786044300343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "f1_list, p_list, r_list = [],[],[]\n",
    "for i in range(3):\n",
    "    print(f\"run ke {i+1}\")\n",
    "    p,r,f = run_bert_for_bencana('angin_topan',i)\n",
    "    f1_list.append(f)\n",
    "    p_list.append(p)\n",
    "    r_list.append(r)\n",
    "display_score_format(f1_list,p_list,r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1d417a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ke 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5599 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5599 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7569 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.7569 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5443 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5443 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7519 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.7519 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5361 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5361 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6938 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6938 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5186 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5186 ACC:0.76 F1:0.02 REC:0.01 PRE:0.40 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7362 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.7362 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4855 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4855 ACC:0.79 F1:0.31 REC:0.19 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5974 ACC:0.69 F1:0.52 REC:0.40 PRE:0.75: 100%|| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.5974 ACC:0.69 F1:0.52 REC:0.40 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4792 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4792 ACC:0.79 F1:0.41 REC:0.31 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7433 ACC:0.62 F1:0.16 REC:0.09 PRE:1.00: 100%|| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.7433 ACC:0.62 F1:0.16 REC:0.09 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4496 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4496 ACC:0.81 F1:0.49 REC:0.37 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5646 ACC:0.74 F1:0.64 REC:0.56 PRE:0.76: 100%|| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.5646 ACC:0.74 F1:0.64 REC:0.56 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4403 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4403 ACC:0.82 F1:0.54 REC:0.44 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5506 ACC:0.76 F1:0.68 REC:0.62 PRE:0.76: 100%|| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.5506 ACC:0.76 F1:0.68 REC:0.62 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4275 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4275 ACC:0.83 F1:0.57 REC:0.49 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5850 ACC:0.73 F1:0.58 REC:0.44 PRE:0.83: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.5850 ACC:0.73 F1:0.58 REC:0.44 PRE:0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4251 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4251 ACC:0.82 F1:0.56 REC:0.47 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5547 ACC:0.77 F1:0.66 REC:0.53 PRE:0.86: 100%|| 5/5 [00:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.5547 ACC:0.77 F1:0.66 REC:0.53 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4124 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4124 ACC:0.84 F1:0.62 REC:0.54 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5547 ACC:0.75 F1:0.63 REC:0.51 PRE:0.82: 100%|| 5/5 [00:04<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.5547 ACC:0.75 F1:0.63 REC:0.51 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4176 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4176 ACC:0.83 F1:0.59 REC:0.52 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5414 ACC:0.78 F1:0.70 REC:0.62 PRE:0.80: 100%|| 5/5 [00:05<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5414 ACC:0.78 F1:0.70 REC:0.62 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4079 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4079 ACC:0.84 F1:0.61 REC:0.53 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5738 ACC:0.75 F1:0.62 REC:0.49 PRE:0.85: 100%|| 5/5 [00:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.5738 ACC:0.75 F1:0.62 REC:0.49 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3884 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3884 ACC:0.85 F1:0.65 REC:0.57 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5981 ACC:0.72 F1:0.53 REC:0.38 PRE:0.89: 100%|| 5/5 [00:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5981 ACC:0.72 F1:0.53 REC:0.38 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3970 LR:0.00000300: 100%|| 39/39 [00:09<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3970 ACC:0.84 F1:0.63 REC:0.57 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6588 ACC:0.68 F1:0.41 REC:0.27 PRE:0.86: 100%|| 5/5 [00:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.6588 ACC:0.68 F1:0.41 REC:0.27 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3813 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3813 ACC:0.85 F1:0.65 REC:0.58 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5400 ACC:0.75 F1:0.65 REC:0.56 PRE:0.78: 100%|| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.5400 ACC:0.75 F1:0.65 REC:0.56 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3971 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3971 ACC:0.84 F1:0.63 REC:0.57 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6451 ACC:0.68 F1:0.41 REC:0.27 PRE:0.86: 100%|| 5/5 [00:05<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.6451 ACC:0.68 F1:0.41 REC:0.27 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3729 LR:0.00000300: 100%|| 39/39 [00:09<00:00,  4.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3729 ACC:0.86 F1:0.67 REC:0.60 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6765 ACC:0.68 F1:0.41 REC:0.27 PRE:0.86: 100%|| 5/5 [00:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.6765 ACC:0.68 F1:0.41 REC:0.27 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3840 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3840 ACC:0.85 F1:0.66 REC:0.60 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6981 ACC:0.68 F1:0.41 REC:0.27 PRE:0.86: 100%|| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.6981 ACC:0.68 F1:0.41 REC:0.27 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3634 LR:0.00000300: 100%|| 39/39 [00:09<00:00,  4.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3634 ACC:0.87 F1:0.70 REC:0.62 PRE:0.79 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5840 ACC:0.76 F1:0.64 REC:0.51 PRE:0.85: 100%|| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.5840 ACC:0.76 F1:0.64 REC:0.51 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3758 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3758 ACC:0.86 F1:0.68 REC:0.61 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7411 ACC:0.69 F1:0.41 REC:0.27 PRE:0.92: 100%|| 5/5 [00:05<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.7411 ACC:0.69 F1:0.41 REC:0.27 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3690 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3690 ACC:0.87 F1:0.69 REC:0.62 PRE:0.79 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5255 ACC:0.76 F1:0.69 REC:0.64 PRE:0.74: 100%|| 5/5 [00:05<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.5255 ACC:0.76 F1:0.69 REC:0.64 PRE:0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3715 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3715 ACC:0.86 F1:0.67 REC:0.59 PRE:0.78 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5409 ACC:0.76 F1:0.68 REC:0.62 PRE:0.76: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.5409 ACC:0.76 F1:0.68 REC:0.62 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3599 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3599 ACC:0.87 F1:0.70 REC:0.65 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6399 ACC:0.68 F1:0.43 REC:0.29 PRE:0.81: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.6399 ACC:0.68 F1:0.43 REC:0.29 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:05<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana banjir\n",
      "Nilai Precision = 0.6818181818181818\n",
      "Nilai Recall = 0.2777777777777778\n",
      "Nilai F1 = 0.39473684210526316\n",
      "run ke 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5689 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5689 ACC:0.76 F1:0.03 REC:0.02 PRE:0.27 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7443 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.7443 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5453 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5453 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7589 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.7589 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5462 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5462 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7561 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:06<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.7561 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5448 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5448 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7682 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.7682 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5402 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5402 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7157 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.7157 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.5256 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.5256 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7620 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.7620 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.5053 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.5053 ACC:0.77 F1:0.14 REC:0.08 PRE:0.60 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7035 ACC:0.62 F1:0.16 REC:0.09 PRE:1.00: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.7035 ACC:0.62 F1:0.16 REC:0.09 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4911 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4911 ACC:0.79 F1:0.32 REC:0.20 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6844 ACC:0.64 F1:0.24 REC:0.13 PRE:1.00: 100%|| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.6844 ACC:0.64 F1:0.24 REC:0.13 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4797 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4797 ACC:0.79 F1:0.44 REC:0.36 PRE:0.58 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6108 ACC:0.69 F1:0.44 REC:0.29 PRE:0.93: 100%|| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.6108 ACC:0.69 F1:0.44 REC:0.29 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4551 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4551 ACC:0.81 F1:0.49 REC:0.38 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5520 ACC:0.78 F1:0.66 REC:0.51 PRE:0.92: 100%|| 5/5 [00:05<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.5520 ACC:0.78 F1:0.66 REC:0.51 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4546 LR:0.00000300: 100%|| 39/39 [00:16<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4546 ACC:0.81 F1:0.52 REC:0.44 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6480 ACC:0.67 F1:0.33 REC:0.20 PRE:1.00: 100%|| 5/5 [00:15<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.6480 ACC:0.67 F1:0.33 REC:0.20 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4349 LR:0.00000300: 100%|| 39/39 [00:40<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4349 ACC:0.84 F1:0.58 REC:0.49 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5279 ACC:0.80 F1:0.70 REC:0.58 PRE:0.90: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5279 ACC:0.80 F1:0.70 REC:0.58 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4343 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4343 ACC:0.83 F1:0.58 REC:0.50 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5516 ACC:0.77 F1:0.64 REC:0.49 PRE:0.92: 100%|| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.5516 ACC:0.77 F1:0.64 REC:0.49 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4212 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4212 ACC:0.84 F1:0.62 REC:0.53 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5361 ACC:0.80 F1:0.69 REC:0.56 PRE:0.93: 100%|| 5/5 [00:15<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5361 ACC:0.80 F1:0.69 REC:0.56 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4139 LR:0.00000300: 100%|| 39/39 [00:20<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4139 ACC:0.85 F1:0.63 REC:0.56 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5229 ACC:0.80 F1:0.71 REC:0.60 PRE:0.87: 100%|| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.5229 ACC:0.80 F1:0.71 REC:0.60 PRE:0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4005 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4005 ACC:0.86 F1:0.67 REC:0.59 PRE:0.78 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5249 ACC:0.81 F1:0.72 REC:0.60 PRE:0.90: 100%|| 5/5 [00:05<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.5249 ACC:0.81 F1:0.72 REC:0.60 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3960 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3960 ACC:0.85 F1:0.64 REC:0.55 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5236 ACC:0.75 F1:0.70 REC:0.69 PRE:0.70: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.5236 ACC:0.75 F1:0.70 REC:0.69 PRE:0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4166 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4166 ACC:0.84 F1:0.64 REC:0.59 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5235 ACC:0.80 F1:0.72 REC:0.62 PRE:0.85: 100%|| 5/5 [00:05<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.5235 ACC:0.80 F1:0.72 REC:0.62 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.4115 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.4115 ACC:0.85 F1:0.63 REC:0.55 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5391 ACC:0.77 F1:0.65 REC:0.51 PRE:0.88: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.5391 ACC:0.77 F1:0.65 REC:0.51 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.4071 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.4071 ACC:0.85 F1:0.62 REC:0.52 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5130 ACC:0.78 F1:0.71 REC:0.67 PRE:0.77: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.5130 ACC:0.78 F1:0.71 REC:0.67 PRE:0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.4002 LR:0.00000300: 100%|| 39/39 [00:26<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.4002 ACC:0.84 F1:0.62 REC:0.53 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5177 ACC:0.78 F1:0.71 REC:0.67 PRE:0.77: 100%|| 5/5 [00:10<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.5177 ACC:0.78 F1:0.71 REC:0.67 PRE:0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3978 LR:0.00000300: 100%|| 39/39 [00:31<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3978 ACC:0.85 F1:0.63 REC:0.56 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5110 ACC:0.79 F1:0.73 REC:0.69 PRE:0.78: 100%|| 5/5 [00:12<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.5110 ACC:0.79 F1:0.73 REC:0.69 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3947 LR:0.00000300: 100%|| 39/39 [00:29<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3947 ACC:0.85 F1:0.65 REC:0.57 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5552 ACC:0.75 F1:0.61 REC:0.47 PRE:0.88: 100%|| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.5552 ACC:0.75 F1:0.61 REC:0.47 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3943 LR:0.00000300: 100%|| 39/39 [00:31<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3943 ACC:0.86 F1:0.66 REC:0.58 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5553 ACC:0.75 F1:0.62 REC:0.49 PRE:0.85: 100%|| 5/5 [00:12<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5553 ACC:0.75 F1:0.62 REC:0.49 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:12<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana banjir\n",
      "Nilai Precision = 0.5833333333333334\n",
      "Nilai Recall = 0.3888888888888889\n",
      "Nilai F1 = 0.4666666666666666\n",
      "run ke 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5715 LR:0.00000300: 100%|| 39/39 [00:22<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5715 ACC:0.76 F1:0.03 REC:0.01 PRE:0.43 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7478 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.7478 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5469 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5469 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7524 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.7524 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5382 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5382 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7535 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.7535 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5298 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5298 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6576 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6576 ACC:0.58 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4992 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4992 ACC:0.77 F1:0.15 REC:0.09 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6142 ACC:0.61 F1:0.19 REC:0.11 PRE:0.71: 100%|| 5/5 [00:06<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6142 ACC:0.61 F1:0.19 REC:0.11 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4899 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4899 ACC:0.78 F1:0.34 REC:0.24 PRE:0.60 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7359 ACC:0.59 F1:0.08 REC:0.04 PRE:0.67: 100%|| 5/5 [00:05<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.7359 ACC:0.59 F1:0.08 REC:0.04 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4510 LR:0.00000300: 100%|| 39/39 [00:33<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4510 ACC:0.81 F1:0.51 REC:0.42 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6436 ACC:0.64 F1:0.34 REC:0.22 PRE:0.71: 100%|| 5/5 [00:13<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.6436 ACC:0.64 F1:0.34 REC:0.22 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4367 LR:0.00000300: 100%|| 39/39 [00:31<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4367 ACC:0.83 F1:0.58 REC:0.49 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6680 ACC:0.64 F1:0.32 REC:0.20 PRE:0.75: 100%|| 5/5 [00:15<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.6680 ACC:0.64 F1:0.32 REC:0.20 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4380 LR:0.00000300: 100%|| 39/39 [00:31<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4380 ACC:0.83 F1:0.59 REC:0.52 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6048 ACC:0.70 F1:0.52 REC:0.38 PRE:0.81: 100%|| 5/5 [00:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.6048 ACC:0.70 F1:0.52 REC:0.38 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4437 LR:0.00000300: 100%|| 39/39 [00:31<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4437 ACC:0.82 F1:0.55 REC:0.45 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5437 ACC:0.78 F1:0.69 REC:0.60 PRE:0.82: 100%|| 5/5 [00:13<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.5437 ACC:0.78 F1:0.69 REC:0.60 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4276 LR:0.00000300: 100%|| 39/39 [00:13<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4276 ACC:0.83 F1:0.56 REC:0.47 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5560 ACC:0.77 F1:0.68 REC:0.60 PRE:0.79: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.5560 ACC:0.77 F1:0.68 REC:0.60 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4182 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4182 ACC:0.83 F1:0.60 REC:0.53 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5619 ACC:0.77 F1:0.68 REC:0.58 PRE:0.81: 100%|| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5619 ACC:0.77 F1:0.68 REC:0.58 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4077 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4077 ACC:0.85 F1:0.62 REC:0.52 PRE:0.77 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5695 ACC:0.78 F1:0.69 REC:0.60 PRE:0.82: 100%|| 5/5 [00:07<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.5695 ACC:0.78 F1:0.69 REC:0.60 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4090 LR:0.00000300: 100%|| 39/39 [00:32<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4090 ACC:0.84 F1:0.61 REC:0.53 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5842 ACC:0.74 F1:0.62 REC:0.51 PRE:0.79: 100%|| 5/5 [00:12<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5842 ACC:0.74 F1:0.62 REC:0.51 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3954 LR:0.00000300: 100%|| 39/39 [00:31<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3954 ACC:0.85 F1:0.65 REC:0.57 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5572 ACC:0.72 F1:0.64 REC:0.60 PRE:0.69: 100%|| 5/5 [00:11<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.5572 ACC:0.72 F1:0.64 REC:0.60 PRE:0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3989 LR:0.00000300: 100%|| 39/39 [00:31<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3989 ACC:0.85 F1:0.64 REC:0.57 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5829 ACC:0.73 F1:0.59 REC:0.47 PRE:0.81: 100%|| 5/5 [00:14<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.5829 ACC:0.73 F1:0.59 REC:0.47 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.4053 LR:0.00000300: 100%|| 39/39 [00:30<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.4053 ACC:0.84 F1:0.62 REC:0.54 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5538 ACC:0.75 F1:0.69 REC:0.67 PRE:0.71: 100%|| 5/5 [00:13<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.5538 ACC:0.75 F1:0.69 REC:0.67 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3921 LR:0.00000300: 100%|| 39/39 [00:28<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3921 ACC:0.85 F1:0.64 REC:0.55 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5573 ACC:0.76 F1:0.70 REC:0.67 PRE:0.73: 100%|| 5/5 [00:05<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.5573 ACC:0.76 F1:0.70 REC:0.67 PRE:0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3679 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3679 ACC:0.87 F1:0.69 REC:0.60 PRE:0.81 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6515 ACC:0.70 F1:0.52 REC:0.38 PRE:0.81: 100%|| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.6515 ACC:0.70 F1:0.52 REC:0.38 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3725 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3725 ACC:0.87 F1:0.68 REC:0.59 PRE:0.79 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6239 ACC:0.71 F1:0.54 REC:0.40 PRE:0.82: 100%|| 5/5 [00:05<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.6239 ACC:0.71 F1:0.54 REC:0.40 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3873 LR:0.00000300: 100%|| 39/39 [00:08<00:00,  4.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3873 ACC:0.86 F1:0.65 REC:0.56 PRE:0.78 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5507 ACC:0.81 F1:0.73 REC:0.62 PRE:0.88: 100%|| 5/5 [00:06<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.5507 ACC:0.81 F1:0.73 REC:0.62 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3737 LR:0.00000300: 100%|| 39/39 [00:28<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3737 ACC:0.87 F1:0.67 REC:0.57 PRE:0.81 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5731 ACC:0.73 F1:0.68 REC:0.69 PRE:0.67: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.5731 ACC:0.73 F1:0.68 REC:0.69 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3641 LR:0.00000300: 100%|| 39/39 [00:32<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3641 ACC:0.87 F1:0.68 REC:0.61 PRE:0.77 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5602 ACC:0.75 F1:0.68 REC:0.64 PRE:0.72: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.5602 ACC:0.75 F1:0.68 REC:0.64 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3604 LR:0.00000300: 100%|| 39/39 [00:30<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3604 ACC:0.87 F1:0.69 REC:0.61 PRE:0.80 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5677 ACC:0.75 F1:0.69 REC:0.67 PRE:0.71: 100%|| 5/5 [00:14<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5677 ACC:0.75 F1:0.69 REC:0.67 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:12<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana banjir\n",
      "Nilai Precision = 0.6226415094339622\n",
      "Nilai Recall = 0.6111111111111112\n",
      "Nilai F1 = 0.616822429906542\n",
      "Nilai maksimal dari semua run:\n",
      "P = 0.6818181818181818\n",
      "R = 0.6111111111111112\n",
      "F1 = 0.616822429906542\n",
      "Nilai minimal dari semua run:\n",
      "P = 0.5833333333333334\n",
      "R = 0.2777777777777778\n",
      "F1 = 0.39473684210526316\n",
      "Nilai rata-rata dari semua run:\n",
      "P = 0.6292643415284925\n",
      "R = 0.425925925925926\n",
      "F1 = 0.4927419795594907\n"
     ]
    }
   ],
   "source": [
    "f1_list, p_list, r_list = [],[],[]\n",
    "for i in range(3):\n",
    "    print(f\"run ke {i+1}\")\n",
    "    p,r,f = run_bert_for_bencana('banjir',i)\n",
    "    f1_list.append(f)\n",
    "    p_list.append(p)\n",
    "    r_list.append(r)\n",
    "display_score_format(f1_list,p_list,r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85e8b32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ke 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.4671 LR:0.00000300: 100%|| 33/33 [00:31<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.4671 ACC:0.87 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4908 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.4908 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.3967 LR:0.00000300: 100%|| 33/33 [00:07<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.3967 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4890 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.4890 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.3897 LR:0.00000300: 100%|| 33/33 [00:08<00:00,  3.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.3897 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4891 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.4891 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.3863 LR:0.00000300: 100%|| 33/33 [00:20<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.3863 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4894 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.4894 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.3841 LR:0.00000300: 100%|| 33/33 [00:29<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.3841 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4896 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:11<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.4896 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3813 LR:0.00000300: 100%|| 33/33 [00:30<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3813 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4902 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:11<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.4902 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.3794 LR:0.00000300: 100%|| 33/33 [00:32<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.3794 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4905 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.4905 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.3776 LR:0.00000300: 100%|| 33/33 [00:32<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.3776 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4910 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.4910 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.3794 LR:0.00000300: 100%|| 33/33 [00:26<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.3794 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4913 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:08<00:00,  1.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.4913 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.3786 LR:0.00000300: 100%|| 33/33 [00:22<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.3786 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4919 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:09<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.4919 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.3767 LR:0.00000300: 100%|| 33/33 [00:23<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.3767 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4922 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:09<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.4922 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.3758 LR:0.00000300: 100%|| 33/33 [00:11<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.3758 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4926 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.4926 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.3754 LR:0.00000300: 100%|| 33/33 [00:07<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.3754 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4930 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.4930 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3743 LR:0.00000300: 100%|| 33/33 [00:07<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3743 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4935 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.4935 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3732 LR:0.00000300: 100%|| 33/33 [00:07<00:00,  4.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3732 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4939 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:09<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.4939 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3731 LR:0.00000300: 100%|| 33/33 [00:26<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3731 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4942 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:08<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.4942 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3727 LR:0.00000300: 100%|| 33/33 [00:23<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3727 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4946 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.4946 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3719 LR:0.00000300: 100%|| 33/33 [00:27<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3719 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4951 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:09<00:00,  1.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.4951 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3703 LR:0.00000300: 100%|| 33/33 [00:25<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3703 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4899 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:09<00:00,  1.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.4899 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3665 LR:0.00000300: 100%|| 33/33 [00:25<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3665 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4820 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.4820 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3583 LR:0.00000300: 100%|| 33/33 [00:30<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3583 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4917 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.4917 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3483 LR:0.00000300: 100%|| 33/33 [00:32<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3483 ACC:0.88 F1:0.06 REC:0.03 PRE:0.43 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4368 ACC:0.80 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:11<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.4368 ACC:0.80 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3310 LR:0.00000300: 100%|| 33/33 [00:32<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3310 ACC:0.90 F1:0.33 REC:0.21 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4507 ACC:0.80 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.4507 ACC:0.80 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3279 LR:0.00000300: 100%|| 33/33 [00:35<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3279 ACC:0.89 F1:0.33 REC:0.22 PRE:0.62 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4311 ACC:0.80 F1:0.09 REC:0.06 PRE:0.25: 100%|| 5/5 [00:10<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.4311 ACC:0.80 F1:0.09 REC:0.06 PRE:0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:11<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana erupsi\n",
      "Nilai Precision = 0.875\n",
      "Nilai Recall = 0.20588235294117646\n",
      "Nilai F1 = 0.33333333333333337\n",
      "run ke 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5147 LR:0.00000300: 100%|| 33/33 [00:37<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5147 ACC:0.85 F1:0.03 REC:0.02 PRE:0.06 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5017 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:13<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.5017 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4259 LR:0.00000300: 100%|| 33/33 [00:35<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4259 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4926 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.4926 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4121 LR:0.00000300: 100%|| 33/33 [00:35<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4121 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4906 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.4906 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4052 LR:0.00000300: 100%|| 33/33 [00:34<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4052 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4897 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:18<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.4897 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4015 LR:0.00000300: 100%|| 33/33 [00:40<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4015 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4892 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.4892 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3981 LR:0.00000300: 100%|| 33/33 [00:33<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3981 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4888 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.4888 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.3961 LR:0.00000300: 100%|| 33/33 [00:36<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.3961 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4887 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.4887 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.3921 LR:0.00000300: 100%|| 33/33 [00:36<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.3921 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4884 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.4884 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.3914 LR:0.00000300: 100%|| 33/33 [00:28<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.3914 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4885 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.4885 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.3878 LR:0.00000300: 100%|| 33/33 [00:29<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.3878 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4883 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:07<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.4883 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.3880 LR:0.00000300: 100%|| 33/33 [00:07<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.3880 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4885 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.4885 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.3866 LR:0.00000300: 100%|| 33/33 [00:07<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.3866 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4886 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.4886 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.3856 LR:0.00000300: 100%|| 33/33 [00:08<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.3856 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4887 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.4887 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3850 LR:0.00000300: 100%|| 33/33 [00:07<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3850 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4889 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:05<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.4889 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3826 LR:0.00000300: 100%|| 33/33 [00:21<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3826 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4890 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:11<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.4890 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3817 LR:0.00000300: 100%|| 33/33 [00:24<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3817 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4893 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:11<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.4893 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3821 LR:0.00000300: 100%|| 33/33 [00:33<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3821 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4895 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.4895 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3800 LR:0.00000300: 100%|| 33/33 [00:38<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3800 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4899 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.4899 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3784 LR:0.00000300: 100%|| 33/33 [00:38<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3784 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4901 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.4901 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3778 LR:0.00000300: 100%|| 33/33 [00:40<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3778 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4903 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:18<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.4903 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3789 LR:0.00000300: 100%|| 33/33 [00:38<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3789 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4907 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.4907 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3759 LR:0.00000300: 100%|| 33/33 [00:39<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3759 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4908 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.4908 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3753 LR:0.00000300: 100%|| 33/33 [00:37<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3753 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4910 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.4910 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3742 LR:0.00000300: 100%|| 33/33 [00:44<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3742 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4915 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.4915 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:18<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana erupsi\n",
      "Nilai Precision = 0.0\n",
      "Nilai Recall = 0.0\n",
      "Nilai F1 = 0.0\n",
      "run ke 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5072 LR:0.00000300: 100%|| 33/33 [00:46<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5072 ACC:0.81 F1:0.11 REC:0.09 PRE:0.13 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4936 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.4936 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4025 LR:0.00000300: 100%|| 33/33 [00:39<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4025 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4878 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:17<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.4878 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.3864 LR:0.00000300: 100%|| 33/33 [00:38<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.3864 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4889 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.4889 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.3818 LR:0.00000300: 100%|| 33/33 [00:38<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.3818 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4902 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.4902 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.3793 LR:0.00000300: 100%|| 33/33 [00:37<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.3793 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4915 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:17<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.4915 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3758 LR:0.00000300: 100%|| 33/33 [00:44<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.3758 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4922 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.4922 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.3759 LR:0.00000300: 100%|| 33/33 [00:37<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.3759 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4935 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.4935 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.3721 LR:0.00000300: 100%|| 33/33 [00:26<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.3721 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4930 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.4930 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.3724 LR:0.00000300: 100%|| 33/33 [00:26<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.3724 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4922 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.4922 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.3711 LR:0.00000300: 100%|| 33/33 [00:27<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.3711 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4929 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:06<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.4929 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.3713 LR:0.00000300: 100%|| 33/33 [00:09<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.3713 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4894 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:06<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.4894 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.3666 LR:0.00000300: 100%|| 33/33 [00:27<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.3666 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4783 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.4783 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.3569 LR:0.00000300: 100%|| 33/33 [00:31<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.3569 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4375 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:13<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.4375 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3460 LR:0.00000300: 100%|| 33/33 [00:38<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3460 ACC:0.88 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4146 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.4146 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3336 LR:0.00000300: 100%|| 33/33 [00:30<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3336 ACC:0.89 F1:0.24 REC:0.14 PRE:0.87 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4913 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.4913 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3450 LR:0.00000300: 100%|| 33/33 [00:39<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3450 ACC:0.88 F1:0.23 REC:0.15 PRE:0.52 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4630 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.4630 ACC:0.82 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3391 LR:0.00000300: 100%|| 33/33 [00:38<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3391 ACC:0.89 F1:0.26 REC:0.17 PRE:0.59 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3995 ACC:0.82 F1:0.18 REC:0.11 PRE:0.50: 100%|| 5/5 [00:15<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.3995 ACC:0.82 F1:0.18 REC:0.11 PRE:0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3084 LR:0.00000300: 100%|| 33/33 [00:39<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3084 ACC:0.90 F1:0.46 REC:0.36 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3950 ACC:0.83 F1:0.26 REC:0.17 PRE:0.60: 100%|| 5/5 [00:17<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.3950 ACC:0.83 F1:0.26 REC:0.17 PRE:0.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3071 LR:0.00000300: 100%|| 33/33 [00:37<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3071 ACC:0.90 F1:0.43 REC:0.31 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3872 ACC:0.80 F1:0.23 REC:0.17 PRE:0.38: 100%|| 5/5 [00:15<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.3872 ACC:0.80 F1:0.23 REC:0.17 PRE:0.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.2984 LR:0.00000300: 100%|| 33/33 [00:35<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.2984 ACC:0.90 F1:0.48 REC:0.37 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3886 ACC:0.80 F1:0.38 REC:0.33 PRE:0.43: 100%|| 5/5 [00:15<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.3886 ACC:0.80 F1:0.38 REC:0.33 PRE:0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3269 LR:0.00000300: 100%|| 33/33 [00:36<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3269 ACC:0.89 F1:0.31 REC:0.20 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3881 ACC:0.80 F1:0.38 REC:0.33 PRE:0.43: 100%|| 5/5 [00:17<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.3881 ACC:0.80 F1:0.38 REC:0.33 PRE:0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.2943 LR:0.00000300: 100%|| 33/33 [00:39<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.2943 ACC:0.91 F1:0.52 REC:0.41 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3850 ACC:0.82 F1:0.31 REC:0.22 PRE:0.50: 100%|| 5/5 [00:18<00:00,  3.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.3850 ACC:0.82 F1:0.31 REC:0.22 PRE:0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.2915 LR:0.00000300: 100%|| 33/33 [00:45<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.2915 ACC:0.90 F1:0.47 REC:0.36 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3989 ACC:0.82 F1:0.18 REC:0.11 PRE:0.50: 100%|| 5/5 [00:17<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.3989 ACC:0.82 F1:0.18 REC:0.11 PRE:0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3107 LR:0.00000300: 100%|| 33/33 [00:40<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3107 ACC:0.89 F1:0.41 REC:0.33 PRE:0.56 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3973 ACC:0.82 F1:0.18 REC:0.11 PRE:0.50: 100%|| 5/5 [00:16<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.3973 ACC:0.82 F1:0.18 REC:0.11 PRE:0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:16<00:00,  1.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana erupsi\n",
      "Nilai Precision = 0.8\n",
      "Nilai Recall = 0.11764705882352941\n",
      "Nilai F1 = 0.20512820512820512\n",
      "Nilai maksimal dari semua run:\n",
      "P = 0.875\n",
      "R = 0.20588235294117646\n",
      "F1 = 0.33333333333333337\n",
      "Nilai minimal dari semua run:\n",
      "P = 0.0\n",
      "R = 0.0\n",
      "F1 = 0.0\n",
      "Nilai rata-rata dari semua run:\n",
      "P = 0.5583333333333333\n",
      "R = 0.10784313725490195\n",
      "F1 = 0.17948717948717952\n"
     ]
    }
   ],
   "source": [
    "f1_list, p_list, r_list = [],[],[]\n",
    "for i in range(3):\n",
    "    print(f\"run ke {i+1}\")\n",
    "    p,r,f = run_bert_for_bencana('erupsi',i)\n",
    "    f1_list.append(f)\n",
    "    p_list.append(p)\n",
    "    r_list.append(r)\n",
    "display_score_format(f1_list,p_list,r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5d8aa810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ke 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5131 LR:0.00000300: 100%|| 39/39 [00:46<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5131 ACC:0.81 F1:0.01 REC:0.01 PRE:1.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6208 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:19<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.6208 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4910 LR:0.00000300: 100%|| 39/39 [00:47<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4910 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6278 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.6278 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4879 LR:0.00000300: 100%|| 39/39 [00:47<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4879 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6265 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6265 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4947 LR:0.00000300: 100%|| 39/39 [00:45<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4947 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6290 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:20<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6290 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4788 LR:0.00000300: 100%|| 39/39 [00:46<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4788 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6232 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:17<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6232 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4761 LR:0.00000300: 100%|| 39/39 [00:40<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4761 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5869 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.5869 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4785 LR:0.00000300: 100%|| 39/39 [00:29<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4785 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5717 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:07<00:00,  1.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.5717 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4619 LR:0.00000300: 100%|| 39/39 [00:13<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4619 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5181 ACC:0.66 F1:0.06 REC:0.03 PRE:1.00: 100%|| 5/5 [00:09<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.5181 ACC:0.66 F1:0.06 REC:0.03 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4383 LR:0.00000300: 100%|| 39/39 [00:32<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4383 ACC:0.81 F1:0.08 REC:0.04 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5121 ACC:0.68 F1:0.16 REC:0.09 PRE:1.00: 100%|| 5/5 [00:14<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.5121 ACC:0.68 F1:0.16 REC:0.09 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4206 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4206 ACC:0.82 F1:0.36 REC:0.26 PRE:0.57 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5475 ACC:0.70 F1:0.25 REC:0.14 PRE:1.00: 100%|| 5/5 [00:16<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.5475 ACC:0.70 F1:0.25 REC:0.14 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4149 LR:0.00000300: 100%|| 39/39 [00:42<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4149 ACC:0.83 F1:0.36 REC:0.25 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5075 ACC:0.70 F1:0.29 REC:0.17 PRE:0.86: 100%|| 5/5 [00:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.5075 ACC:0.70 F1:0.29 REC:0.17 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.3868 LR:0.00000300: 100%|| 39/39 [00:42<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.3868 ACC:0.87 F1:0.55 REC:0.42 PRE:0.78 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5354 ACC:0.70 F1:0.32 REC:0.20 PRE:0.78: 100%|| 5/5 [00:15<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5354 ACC:0.70 F1:0.32 REC:0.20 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4023 LR:0.00000300: 100%|| 39/39 [00:46<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4023 ACC:0.84 F1:0.44 REC:0.33 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4465 ACC:0.74 F1:0.50 REC:0.37 PRE:0.76: 100%|| 5/5 [00:15<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.4465 ACC:0.74 F1:0.50 REC:0.37 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3880 LR:0.00000300: 100%|| 39/39 [00:46<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3880 ACC:0.85 F1:0.51 REC:0.41 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4335 ACC:0.76 F1:0.57 REC:0.46 PRE:0.76: 100%|| 5/5 [00:15<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.4335 ACC:0.76 F1:0.57 REC:0.46 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3857 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3857 ACC:0.85 F1:0.53 REC:0.45 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5690 ACC:0.70 F1:0.25 REC:0.14 PRE:1.00: 100%|| 5/5 [00:14<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.5690 ACC:0.70 F1:0.25 REC:0.14 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3859 LR:0.00000300: 100%|| 39/39 [00:48<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3859 ACC:0.84 F1:0.49 REC:0.39 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5185 ACC:0.72 F1:0.36 REC:0.23 PRE:0.89: 100%|| 5/5 [00:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.5185 ACC:0.72 F1:0.36 REC:0.23 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3680 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3680 ACC:0.86 F1:0.57 REC:0.47 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5175 ACC:0.72 F1:0.36 REC:0.23 PRE:0.89: 100%|| 5/5 [00:17<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.5175 ACC:0.72 F1:0.36 REC:0.23 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3621 LR:0.00000300: 100%|| 39/39 [00:48<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3621 ACC:0.87 F1:0.60 REC:0.50 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4430 ACC:0.76 F1:0.57 REC:0.46 PRE:0.76: 100%|| 5/5 [00:19<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.4430 ACC:0.76 F1:0.57 REC:0.46 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3600 LR:0.00000300: 100%|| 39/39 [00:51<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3600 ACC:0.88 F1:0.62 REC:0.53 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4171 ACC:0.77 F1:0.63 REC:0.57 PRE:0.71: 100%|| 5/5 [00:18<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.4171 ACC:0.77 F1:0.63 REC:0.57 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3745 LR:0.00000300: 100%|| 39/39 [00:48<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3745 ACC:0.85 F1:0.55 REC:0.50 PRE:0.62 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4416 ACC:0.76 F1:0.57 REC:0.46 PRE:0.76: 100%|| 5/5 [00:19<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.4416 ACC:0.76 F1:0.57 REC:0.46 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3527 LR:0.00000300: 100%|| 39/39 [00:48<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3527 ACC:0.87 F1:0.60 REC:0.50 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5240 ACC:0.72 F1:0.36 REC:0.23 PRE:0.89: 100%|| 5/5 [00:15<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.5240 ACC:0.72 F1:0.36 REC:0.23 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3400 LR:0.00000300: 100%|| 39/39 [00:16<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3400 ACC:0.87 F1:0.61 REC:0.53 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4285 ACC:0.76 F1:0.60 REC:0.51 PRE:0.72: 100%|| 5/5 [00:07<00:00,  1.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.4285 ACC:0.76 F1:0.60 REC:0.51 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3496 LR:0.00000300: 100%|| 39/39 [00:12<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3496 ACC:0.86 F1:0.59 REC:0.53 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4689 ACC:0.76 F1:0.50 REC:0.34 PRE:0.92: 100%|| 5/5 [00:06<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.4689 ACC:0.76 F1:0.50 REC:0.34 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3454 LR:0.00000300: 100%|| 39/39 [00:12<00:00,  3.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3454 ACC:0.88 F1:0.62 REC:0.53 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4249 ACC:0.75 F1:0.58 REC:0.49 PRE:0.71: 100%|| 5/5 [00:11<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.4249 ACC:0.75 F1:0.58 REC:0.49 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:18<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana gempa\n",
      "Nilai Precision = 0.6774193548387096\n",
      "Nilai Recall = 0.4666666666666667\n",
      "Nilai F1 = 0.5526315789473684\n",
      "run ke 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5354 LR:0.00000300: 100%|| 39/39 [00:44<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5354 ACC:0.80 F1:0.03 REC:0.02 PRE:0.30 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6112 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.6112 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4885 LR:0.00000300: 100%|| 39/39 [00:48<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4885 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6174 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:20<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.6174 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4876 LR:0.00000300: 100%|| 39/39 [00:51<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4876 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6208 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:19<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6208 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4961 LR:0.00000300: 100%|| 39/39 [00:53<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4961 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6235 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:22<00:00,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6235 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4898 LR:0.00000300: 100%|| 39/39 [00:51<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4898 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6245 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:20<00:00,  4.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6245 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.5005 LR:0.00000300: 100%|| 39/39 [00:47<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.5005 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6244 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.6244 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4841 LR:0.00000300: 100%|| 39/39 [00:49<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4841 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6249 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:17<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.6249 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4827 LR:0.00000300: 100%|| 39/39 [00:44<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4827 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6273 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:18<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.6273 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4879 LR:0.00000300: 100%|| 39/39 [00:44<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4879 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6215 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:17<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.6215 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4876 LR:0.00000300: 100%|| 39/39 [00:47<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4876 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6267 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:18<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.6267 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4872 LR:0.00000300: 100%|| 39/39 [00:51<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4872 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6223 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:18<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.6223 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4836 LR:0.00000300: 100%|| 39/39 [00:48<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4836 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6145 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:20<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.6145 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4639 LR:0.00000300: 100%|| 39/39 [00:47<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4639 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5799 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:18<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.5799 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4589 LR:0.00000300: 100%|| 39/39 [00:41<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4589 ACC:0.81 F1:0.11 REC:0.06 PRE:0.58 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5349 ACC:0.67 F1:0.20 REC:0.11 PRE:0.67: 100%|| 5/5 [00:14<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5349 ACC:0.67 F1:0.20 REC:0.11 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4381 LR:0.00000300: 100%|| 39/39 [00:37<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4381 ACC:0.83 F1:0.31 REC:0.19 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5184 ACC:0.73 F1:0.49 REC:0.37 PRE:0.72: 100%|| 5/5 [00:13<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.5184 ACC:0.73 F1:0.49 REC:0.37 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4410 LR:0.00000300: 100%|| 39/39 [00:33<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4410 ACC:0.82 F1:0.36 REC:0.26 PRE:0.58 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6262 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.6262 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.4162 LR:0.00000300: 100%|| 39/39 [00:34<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.4162 ACC:0.85 F1:0.48 REC:0.35 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5944 ACC:0.67 F1:0.15 REC:0.09 PRE:0.75: 100%|| 5/5 [00:14<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.5944 ACC:0.67 F1:0.15 REC:0.09 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4189 LR:0.00000300: 100%|| 39/39 [00:34<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4189 ACC:0.85 F1:0.48 REC:0.37 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4970 ACC:0.76 F1:0.57 REC:0.46 PRE:0.76: 100%|| 5/5 [00:17<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.4970 ACC:0.76 F1:0.57 REC:0.46 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.4180 LR:0.00000300: 100%|| 39/39 [00:49<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.4180 ACC:0.85 F1:0.42 REC:0.29 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5215 ACC:0.68 F1:0.30 REC:0.20 PRE:0.64: 100%|| 5/5 [00:19<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.5215 ACC:0.68 F1:0.30 REC:0.20 PRE:0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.4050 LR:0.00000300: 100%|| 39/39 [00:47<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.4050 ACC:0.85 F1:0.47 REC:0.34 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5781 ACC:0.67 F1:0.15 REC:0.09 PRE:0.75: 100%|| 5/5 [00:19<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.5781 ACC:0.67 F1:0.15 REC:0.09 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3949 LR:0.00000300: 100%|| 39/39 [00:48<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3949 ACC:0.87 F1:0.56 REC:0.45 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5324 ACC:0.72 F1:0.39 REC:0.26 PRE:0.82: 100%|| 5/5 [00:17<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.5324 ACC:0.72 F1:0.39 REC:0.26 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3942 LR:0.00000300: 100%|| 39/39 [00:51<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3942 ACC:0.85 F1:0.53 REC:0.45 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5423 ACC:0.71 F1:0.33 REC:0.20 PRE:0.88: 100%|| 5/5 [00:19<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.5423 ACC:0.71 F1:0.33 REC:0.20 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.4151 LR:0.00000300: 100%|| 39/39 [00:51<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.4151 ACC:0.84 F1:0.42 REC:0.30 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5130 ACC:0.72 F1:0.39 REC:0.26 PRE:0.82: 100%|| 5/5 [00:17<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.5130 ACC:0.72 F1:0.39 REC:0.26 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3917 LR:0.00000300: 100%|| 39/39 [00:35<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3917 ACC:0.86 F1:0.53 REC:0.42 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5344 ACC:0.70 F1:0.29 REC:0.17 PRE:0.86: 100%|| 5/5 [00:13<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5344 ACC:0.70 F1:0.29 REC:0.17 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:14<00:00,  1.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana gempa\n",
      "Nilai Precision = 0.7142857142857143\n",
      "Nilai Recall = 0.2222222222222222\n",
      "Nilai F1 = 0.3389830508474576\n",
      "run ke 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5248 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5248 ACC:0.80 F1:0.01 REC:0.01 PRE:0.12 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6155 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.6155 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4881 LR:0.00000300: 100%|| 39/39 [00:45<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.4881 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6245 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.6245 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4841 LR:0.00000300: 100%|| 39/39 [00:50<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.4841 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6265 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:19<00:00,  3.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6265 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4988 LR:0.00000300: 100%|| 39/39 [00:51<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.4988 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6223 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:19<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6223 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4879 LR:0.00000300: 100%|| 39/39 [00:52<00:00,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4879 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6219 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:19<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6219 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4798 LR:0.00000300: 100%|| 39/39 [00:47<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4798 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6102 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:18<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.6102 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4693 LR:0.00000300: 100%|| 39/39 [00:41<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4693 ACC:0.81 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6019 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:17<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.6019 ACC:0.65 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4543 LR:0.00000300: 100%|| 39/39 [00:44<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4543 ACC:0.81 F1:0.04 REC:0.02 PRE:0.80 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5712 ACC:0.66 F1:0.06 REC:0.03 PRE:1.00: 100%|| 5/5 [00:18<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.5712 ACC:0.66 F1:0.06 REC:0.03 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4439 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4439 ACC:0.83 F1:0.23 REC:0.14 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5894 ACC:0.67 F1:0.11 REC:0.06 PRE:1.00: 100%|| 5/5 [00:13<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.5894 ACC:0.67 F1:0.11 REC:0.06 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4565 LR:0.00000300: 100%|| 39/39 [00:35<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4565 ACC:0.82 F1:0.28 REC:0.18 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5419 ACC:0.70 F1:0.29 REC:0.17 PRE:0.86: 100%|| 5/5 [00:13<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.5419 ACC:0.70 F1:0.29 REC:0.17 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4222 LR:0.00000300: 100%|| 39/39 [00:37<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4222 ACC:0.84 F1:0.41 REC:0.29 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5334 ACC:0.70 F1:0.32 REC:0.20 PRE:0.78: 100%|| 5/5 [00:22<00:00,  4.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.5334 ACC:0.70 F1:0.32 REC:0.20 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4174 LR:0.00000300: 100%|| 39/39 [00:45<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4174 ACC:0.83 F1:0.41 REC:0.31 PRE:0.60 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5355 ACC:0.72 F1:0.36 REC:0.23 PRE:0.89: 100%|| 5/5 [00:16<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5355 ACC:0.72 F1:0.36 REC:0.23 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4202 LR:0.00000300: 100%|| 39/39 [00:47<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4202 ACC:0.84 F1:0.44 REC:0.33 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4882 ACC:0.75 F1:0.49 REC:0.34 PRE:0.86: 100%|| 5/5 [00:17<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.4882 ACC:0.75 F1:0.49 REC:0.34 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4139 LR:0.00000300: 100%|| 39/39 [00:46<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4139 ACC:0.83 F1:0.43 REC:0.34 PRE:0.59 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5432 ACC:0.70 F1:0.29 REC:0.17 PRE:0.86: 100%|| 5/5 [00:20<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5432 ACC:0.70 F1:0.29 REC:0.17 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3993 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3993 ACC:0.85 F1:0.49 REC:0.38 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4798 ACC:0.76 F1:0.54 REC:0.40 PRE:0.82: 100%|| 5/5 [00:18<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.4798 ACC:0.76 F1:0.54 REC:0.40 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3820 LR:0.00000300: 100%|| 39/39 [00:41<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3820 ACC:0.86 F1:0.54 REC:0.43 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5269 ACC:0.73 F1:0.43 REC:0.29 PRE:0.83: 100%|| 5/5 [00:15<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.5269 ACC:0.73 F1:0.43 REC:0.29 PRE:0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3866 LR:0.00000300: 100%|| 39/39 [00:40<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3866 ACC:0.85 F1:0.53 REC:0.45 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5436 ACC:0.71 F1:0.33 REC:0.20 PRE:0.88: 100%|| 5/5 [00:17<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.5436 ACC:0.71 F1:0.33 REC:0.20 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4038 LR:0.00000300: 100%|| 39/39 [00:49<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4038 ACC:0.84 F1:0.48 REC:0.39 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5117 ACC:0.73 F1:0.43 REC:0.29 PRE:0.83: 100%|| 5/5 [00:18<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.5117 ACC:0.73 F1:0.43 REC:0.29 PRE:0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3808 LR:0.00000300: 100%|| 39/39 [00:49<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3808 ACC:0.85 F1:0.48 REC:0.37 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4626 ACC:0.75 F1:0.56 REC:0.46 PRE:0.73: 100%|| 5/5 [00:19<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.4626 ACC:0.75 F1:0.56 REC:0.46 PRE:0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3788 LR:0.00000300: 100%|| 39/39 [00:50<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3788 ACC:0.86 F1:0.55 REC:0.45 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4758 ACC:0.76 F1:0.56 REC:0.43 PRE:0.79: 100%|| 5/5 [00:18<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.4758 ACC:0.76 F1:0.56 REC:0.43 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3574 LR:0.00000300: 100%|| 39/39 [00:49<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3574 ACC:0.87 F1:0.60 REC:0.53 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5245 ACC:0.73 F1:0.43 REC:0.29 PRE:0.83: 100%|| 5/5 [00:18<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.5245 ACC:0.73 F1:0.43 REC:0.29 PRE:0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3658 LR:0.00000300: 100%|| 39/39 [00:49<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3658 ACC:0.87 F1:0.59 REC:0.49 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4678 ACC:0.74 F1:0.57 REC:0.49 PRE:0.68: 100%|| 5/5 [00:18<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.4678 ACC:0.74 F1:0.57 REC:0.49 PRE:0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3568 LR:0.00000300: 100%|| 39/39 [00:46<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3568 ACC:0.88 F1:0.61 REC:0.49 PRE:0.79 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5308 ACC:0.73 F1:0.40 REC:0.26 PRE:0.90: 100%|| 5/5 [00:19<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.5308 ACC:0.73 F1:0.40 REC:0.26 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3661 LR:0.00000300: 100%|| 39/39 [00:53<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3661 ACC:0.87 F1:0.58 REC:0.47 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5246 ACC:0.73 F1:0.43 REC:0.29 PRE:0.83: 100%|| 5/5 [00:17<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5246 ACC:0.73 F1:0.43 REC:0.29 PRE:0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:18<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana gempa\n",
      "Nilai Precision = 0.9090909090909091\n",
      "Nilai Recall = 0.2222222222222222\n",
      "Nilai F1 = 0.3571428571428571\n",
      "Nilai maksimal dari semua run:\n",
      "P = 0.9090909090909091\n",
      "R = 0.4666666666666667\n",
      "F1 = 0.5526315789473684\n",
      "Nilai minimal dari semua run:\n",
      "P = 0.6774193548387096\n",
      "R = 0.2222222222222222\n",
      "F1 = 0.3389830508474576\n",
      "Nilai rata-rata dari semua run:\n",
      "P = 0.7669319927384444\n",
      "R = 0.3037037037037037\n",
      "F1 = 0.41625249564589434\n"
     ]
    }
   ],
   "source": [
    "f1_list, p_list, r_list = [],[],[]\n",
    "for i in range(3):\n",
    "    print(f\"run ke {i+1}\")\n",
    "    p,r,f = run_bert_for_bencana('gempa',i)\n",
    "    f1_list.append(f)\n",
    "    p_list.append(p)\n",
    "    r_list.append(r)\n",
    "display_score_format(f1_list,p_list,r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21f5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ke 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.6660 LR:0.00000300: 100%|| 36/36 [00:52<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.6660 ACC:0.63 F1:0.04 REC:0.02 PRE:0.19 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7050 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:19<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.7050 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.6368 LR:0.00000300: 100%|| 36/36 [00:45<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.6368 ACC:0.66 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7190 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.7190 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.6346 LR:0.00000300: 100%|| 36/36 [00:37<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.6346 ACC:0.66 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7067 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.7067 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.6440 LR:0.00000300: 100%|| 36/36 [00:36<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.6440 ACC:0.66 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7080 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.7080 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.6348 LR:0.00000300: 100%|| 36/36 [00:33<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.6348 ACC:0.66 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6833 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6833 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.6364 LR:0.00000300: 100%|| 36/36 [00:32<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.6364 ACC:0.66 F1:0.02 REC:0.01 PRE:1.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6566 ACC:0.57 F1:0.08 REC:0.04 PRE:1.00: 100%|| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.6566 ACC:0.57 F1:0.08 REC:0.04 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.6031 LR:0.00000300: 100%|| 36/36 [00:34<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.6031 ACC:0.67 F1:0.18 REC:0.11 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7282 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.7282 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.6050 LR:0.00000300: 100%|| 36/36 [00:41<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.6050 ACC:0.69 F1:0.31 REC:0.20 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6179 ACC:0.69 F1:0.58 REC:0.48 PRE:0.74: 100%|| 5/5 [00:17<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.6179 ACC:0.69 F1:0.58 REC:0.48 PRE:0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.5790 LR:0.00000300: 100%|| 36/36 [00:47<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.5790 ACC:0.72 F1:0.45 REC:0.34 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6026 ACC:0.71 F1:0.69 REC:0.71 PRE:0.67: 100%|| 5/5 [00:17<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.6026 ACC:0.71 F1:0.69 REC:0.71 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.5589 LR:0.00000300: 100%|| 36/36 [00:45<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.5589 ACC:0.73 F1:0.53 REC:0.44 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6162 ACC:0.69 F1:0.72 REC:0.90 PRE:0.61: 100%|| 5/5 [00:17<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.6162 ACC:0.69 F1:0.72 REC:0.90 PRE:0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.5755 LR:0.00000300: 100%|| 36/36 [00:46<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.5755 ACC:0.71 F1:0.52 REC:0.47 PRE:0.59 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6132 ACC:0.67 F1:0.53 REC:0.42 PRE:0.74: 100%|| 5/5 [00:17<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.6132 ACC:0.67 F1:0.53 REC:0.42 PRE:0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.5415 LR:0.00000300: 100%|| 36/36 [00:47<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.5415 ACC:0.74 F1:0.57 REC:0.51 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5732 ACC:0.72 F1:0.69 REC:0.71 PRE:0.68: 100%|| 5/5 [00:19<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5732 ACC:0.72 F1:0.69 REC:0.71 PRE:0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.5411 LR:0.00000300: 100%|| 36/36 [00:47<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.5411 ACC:0.74 F1:0.57 REC:0.49 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5748 ACC:0.75 F1:0.74 REC:0.79 PRE:0.69: 100%|| 5/5 [00:17<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.5748 ACC:0.75 F1:0.74 REC:0.79 PRE:0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.5291 LR:0.00000300: 100%|| 36/36 [00:44<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.5291 ACC:0.76 F1:0.62 REC:0.57 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5678 ACC:0.74 F1:0.71 REC:0.73 PRE:0.70: 100%|| 5/5 [00:17<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5678 ACC:0.74 F1:0.71 REC:0.73 PRE:0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.5322 LR:0.00000300: 100%|| 36/36 [00:42<00:00,  1.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.5322 ACC:0.76 F1:0.62 REC:0.56 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5780 ACC:0.72 F1:0.62 REC:0.52 PRE:0.78: 100%|| 5/5 [00:16<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.5780 ACC:0.72 F1:0.62 REC:0.52 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.5105 LR:0.00000300: 100%|| 36/36 [00:43<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.5105 ACC:0.77 F1:0.64 REC:0.59 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5919 ACC:0.70 F1:0.59 REC:0.48 PRE:0.77: 100%|| 5/5 [00:17<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.5919 ACC:0.70 F1:0.59 REC:0.48 PRE:0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.5275 LR:0.00000300: 100%|| 36/36 [00:47<00:00,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.5275 ACC:0.77 F1:0.62 REC:0.55 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7058 ACC:0.63 F1:0.35 REC:0.23 PRE:0.79: 100%|| 5/5 [00:17<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.7058 ACC:0.63 F1:0.35 REC:0.23 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.5208 LR:0.00000300: 100%|| 36/36 [00:45<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.5208 ACC:0.77 F1:0.62 REC:0.56 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5781 ACC:0.72 F1:0.72 REC:0.81 PRE:0.65: 100%|| 5/5 [00:15<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.5781 ACC:0.72 F1:0.72 REC:0.81 PRE:0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.5083 LR:0.00000300: 100%|| 36/36 [00:45<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.5083 ACC:0.77 F1:0.66 REC:0.63 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6351 ACC:0.64 F1:0.42 REC:0.29 PRE:0.78: 100%|| 5/5 [00:20<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.6351 ACC:0.64 F1:0.42 REC:0.29 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.5245 LR:0.00000300: 100%|| 36/36 [00:47<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.5245 ACC:0.76 F1:0.60 REC:0.52 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5904 ACC:0.72 F1:0.72 REC:0.81 PRE:0.65: 100%|| 5/5 [00:18<00:00,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.5904 ACC:0.72 F1:0.72 REC:0.81 PRE:0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.5090 LR:0.00000300: 100%|| 36/36 [00:40<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.5090 ACC:0.77 F1:0.64 REC:0.59 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5571 ACC:0.79 F1:0.72 REC:0.62 PRE:0.86: 100%|| 5/5 [00:15<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.5571 ACC:0.79 F1:0.72 REC:0.62 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.4823 LR:0.00000300: 100%|| 36/36 [00:36<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.4823 ACC:0.79 F1:0.66 REC:0.61 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5533 ACC:0.79 F1:0.73 REC:0.65 PRE:0.84: 100%|| 5/5 [00:09<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.5533 ACC:0.79 F1:0.73 REC:0.65 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.4856 LR:0.00000300: 100%|| 36/36 [00:09<00:00,  3.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.4856 ACC:0.78 F1:0.66 REC:0.61 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5543 ACC:0.77 F1:0.71 REC:0.62 PRE:0.81: 100%|| 5/5 [00:06<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.5543 ACC:0.77 F1:0.71 REC:0.62 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.4867 LR:0.00000300: 100%|| 36/36 [00:11<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.4867 ACC:0.78 F1:0.65 REC:0.58 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5576 ACC:0.75 F1:0.72 REC:0.73 PRE:0.71: 100%|| 5/5 [00:13<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5576 ACC:0.75 F1:0.72 REC:0.73 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:13<00:00,  1.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana karhutla\n",
      "Nilai Precision = 0.6065573770491803\n",
      "Nilai Recall = 0.6607142857142857\n",
      "Nilai F1 = 0.6324786324786325\n",
      "run ke 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.6509 LR:0.00000300: 100%|| 36/36 [00:32<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.6509 ACC:0.63 F1:0.07 REC:0.04 PRE:0.27 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7122 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.7122 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.6388 LR:0.00000300: 100%|| 36/36 [00:34<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.6388 ACC:0.66 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7129 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.7129 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.6365 LR:0.00000300: 100%|| 36/36 [00:39<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.6365 ACC:0.66 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7121 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.7121 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.6345 LR:0.00000300: 100%|| 36/36 [00:38<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.6345 ACC:0.66 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7148 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.7148 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.6420 LR:0.00000300: 100%|| 36/36 [00:41<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.6420 ACC:0.66 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6857 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6857 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.6449 LR:0.00000300: 100%|| 36/36 [00:41<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.6449 ACC:0.66 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7339 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:16<00:00,  3.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.7339 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.6277 LR:0.00000300: 100%|| 36/36 [00:40<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.6277 ACC:0.66 F1:0.04 REC:0.02 PRE:0.86 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7241 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.7241 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.6287 LR:0.00000300: 100%|| 36/36 [00:41<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.6287 ACC:0.67 F1:0.07 REC:0.04 PRE:0.92 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6811 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.6811 ACC:0.55 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.5956 LR:0.00000300: 100%|| 36/36 [00:43<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.5956 ACC:0.69 F1:0.31 REC:0.20 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6689 ACC:0.63 F1:0.35 REC:0.23 PRE:0.79: 100%|| 5/5 [00:18<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.6689 ACC:0.63 F1:0.35 REC:0.23 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.6246 LR:0.00000300: 100%|| 36/36 [00:46<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.6246 ACC:0.67 F1:0.26 REC:0.16 PRE:0.60 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6460 ACC:0.66 F1:0.45 REC:0.31 PRE:0.83: 100%|| 5/5 [00:18<00:00,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.6460 ACC:0.66 F1:0.45 REC:0.31 PRE:0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.6055 LR:0.00000300: 100%|| 36/36 [00:49<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.6055 ACC:0.70 F1:0.43 REC:0.32 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6057 ACC:0.66 F1:0.57 REC:0.50 PRE:0.67: 100%|| 5/5 [00:17<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.6057 ACC:0.66 F1:0.57 REC:0.50 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.5715 LR:0.00000300: 100%|| 36/36 [00:43<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.5715 ACC:0.71 F1:0.47 REC:0.38 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6087 ACC:0.68 F1:0.53 REC:0.40 PRE:0.79: 100%|| 5/5 [00:17<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.6087 ACC:0.68 F1:0.53 REC:0.40 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.5888 LR:0.00000300: 100%|| 36/36 [00:36<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.5888 ACC:0.70 F1:0.47 REC:0.38 PRE:0.60 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6206 ACC:0.66 F1:0.49 REC:0.35 PRE:0.77: 100%|| 5/5 [00:14<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.6206 ACC:0.66 F1:0.49 REC:0.35 PRE:0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.5442 LR:0.00000300: 100%|| 36/36 [00:34<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.5442 ACC:0.73 F1:0.55 REC:0.48 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7019 ACC:0.66 F1:0.45 REC:0.31 PRE:0.83: 100%|| 5/5 [00:13<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.7019 ACC:0.66 F1:0.45 REC:0.31 PRE:0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.5492 LR:0.00000300: 100%|| 36/36 [00:30<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.5492 ACC:0.74 F1:0.53 REC:0.42 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6566 ACC:0.66 F1:0.47 REC:0.33 PRE:0.80: 100%|| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.6566 ACC:0.66 F1:0.47 REC:0.33 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.5577 LR:0.00000300: 100%|| 36/36 [00:32<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.5577 ACC:0.73 F1:0.56 REC:0.49 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7689 ACC:0.58 F1:0.18 REC:0.10 PRE:0.71: 100%|| 5/5 [00:14<00:00,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.7689 ACC:0.58 F1:0.18 REC:0.10 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.5742 LR:0.00000300: 100%|| 36/36 [00:31<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.5742 ACC:0.71 F1:0.47 REC:0.38 PRE:0.63 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6357 ACC:0.66 F1:0.47 REC:0.33 PRE:0.80: 100%|| 5/5 [00:12<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.6357 ACC:0.66 F1:0.47 REC:0.33 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.5373 LR:0.00000300: 100%|| 36/36 [00:32<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.5373 ACC:0.74 F1:0.54 REC:0.45 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6213 ACC:0.68 F1:0.55 REC:0.44 PRE:0.75: 100%|| 5/5 [00:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.6213 ACC:0.68 F1:0.55 REC:0.44 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.5186 LR:0.00000300: 100%|| 36/36 [00:45<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.5186 ACC:0.77 F1:0.62 REC:0.53 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6248 ACC:0.68 F1:0.57 REC:0.48 PRE:0.72: 100%|| 5/5 [00:17<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.6248 ACC:0.68 F1:0.57 REC:0.48 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.5624 LR:0.00000300: 100%|| 36/36 [00:43<00:00,  1.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.5624 ACC:0.74 F1:0.60 REC:0.56 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6372 ACC:0.67 F1:0.49 REC:0.35 PRE:0.81: 100%|| 5/5 [00:18<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.6372 ACC:0.67 F1:0.49 REC:0.35 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.5209 LR:0.00000300: 100%|| 36/36 [00:41<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.5209 ACC:0.77 F1:0.62 REC:0.56 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6296 ACC:0.68 F1:0.54 REC:0.42 PRE:0.77: 100%|| 5/5 [00:16<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.6296 ACC:0.68 F1:0.54 REC:0.42 PRE:0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.5249 LR:0.00000300: 100%|| 36/36 [00:41<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.5249 ACC:0.77 F1:0.62 REC:0.57 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "f1_list, p_list, r_list = [],[],[]\n",
    "for i in range(3):\n",
    "    print(f\"run ke {i+1}\")\n",
    "    p,r,f = run_bert_for_bencana('karhutla',i)\n",
    "    f1_list.append(f)\n",
    "    p_list.append(p)\n",
    "    r_list.append(r)\n",
    "display_score_format(f1_list,p_list,r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a43d0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ke 2\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.018505096435546875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading ()solve/main/vocab.txt",
       "rate": null,
       "total": 229167,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54fa86892492402cae9c02f849f7b8d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()solve/main/vocab.txt:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.11509037017822266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading ()cial_tokens_map.json",
       "rate": null,
       "total": 112,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92cd3acd3f76405d8e02622f5f917c23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017531394958496094,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading ()okenizer_config.json",
       "rate": null,
       "total": 2,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824743c8f8d4447c8a49e04a11548d64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()okenizer_config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015164852142333984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading ()lve/main/config.json",
       "rate": null,
       "total": 1535,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be6aca34c1f4b6aa28dfbeaefaab180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()lve/main/config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01542520523071289,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 1340657204,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0176fa9750cd47168e0429491a820622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5445 LR:0.00000300: 100%|| 34/34 [00:39<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5445 ACC:0.79 F1:0.01 REC:0.01 PRE:0.25 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5940 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.5940 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5160 LR:0.00000300: 100%|| 34/34 [00:27<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5160 ACC:0.79 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5965 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.5965 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5038 LR:0.00000300: 100%|| 34/34 [00:26<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5038 ACC:0.79 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5939 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:09<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.5939 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5089 LR:0.00000300: 100%|| 34/34 [00:26<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5089 ACC:0.79 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5800 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.5800 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4974 LR:0.00000300: 100%|| 34/34 [00:39<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4974 ACC:0.79 F1:0.01 REC:0.01 PRE:1.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6008 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6008 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4751 LR:0.00000300: 100%|| 34/34 [00:27<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4751 ACC:0.80 F1:0.04 REC:0.02 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5410 ACC:0.72 F1:0.12 REC:0.07 PRE:0.50: 100%|| 5/5 [00:09<00:00,  1.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.5410 ACC:0.72 F1:0.12 REC:0.07 PRE:0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4608 LR:0.00000300: 100%|| 34/34 [00:28<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4608 ACC:0.81 F1:0.25 REC:0.15 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5008 ACC:0.75 F1:0.36 REC:0.25 PRE:0.64: 100%|| 5/5 [00:08<00:00,  1.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.5008 ACC:0.75 F1:0.36 REC:0.25 PRE:0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4287 LR:0.00000300: 100%|| 34/34 [00:23<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4287 ACC:0.83 F1:0.44 REC:0.33 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5897 ACC:0.73 F1:0.13 REC:0.07 PRE:0.67: 100%|| 5/5 [00:07<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.5897 ACC:0.73 F1:0.13 REC:0.07 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4373 LR:0.00000300: 100%|| 34/34 [00:08<00:00,  3.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4373 ACC:0.83 F1:0.43 REC:0.32 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4858 ACC:0.80 F1:0.66 REC:0.68 PRE:0.63: 100%|| 5/5 [00:05<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.4858 ACC:0.80 F1:0.66 REC:0.68 PRE:0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4396 LR:0.00000300: 100%|| 34/34 [00:08<00:00,  3.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4396 ACC:0.83 F1:0.51 REC:0.44 PRE:0.62 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5980 ACC:0.73 F1:0.13 REC:0.07 PRE:0.67: 100%|| 5/5 [00:05<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.5980 ACC:0.73 F1:0.13 REC:0.07 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4341 LR:0.00000300: 100%|| 34/34 [00:08<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4341 ACC:0.83 F1:0.45 REC:0.34 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4577 ACC:0.83 F1:0.69 REC:0.68 PRE:0.70: 100%|| 5/5 [00:06<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.4577 ACC:0.83 F1:0.69 REC:0.68 PRE:0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4074 LR:0.00000300: 100%|| 34/34 [00:30<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4074 ACC:0.84 F1:0.57 REC:0.50 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4560 ACC:0.81 F1:0.58 REC:0.46 PRE:0.76: 100%|| 5/5 [00:11<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.4560 ACC:0.81 F1:0.58 REC:0.46 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.3952 LR:0.00000300: 100%|| 34/34 [00:31<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.3952 ACC:0.86 F1:0.58 REC:0.48 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5133 ACC:0.78 F1:0.45 REC:0.32 PRE:0.75: 100%|| 5/5 [00:10<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.5133 ACC:0.78 F1:0.45 REC:0.32 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3908 LR:0.00000300: 100%|| 34/34 [00:31<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3908 ACC:0.85 F1:0.60 REC:0.52 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4246 ACC:0.85 F1:0.71 REC:0.64 PRE:0.78: 100%|| 5/5 [00:09<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.4246 ACC:0.85 F1:0.71 REC:0.64 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3767 LR:0.00000300: 100%|| 34/34 [00:32<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3767 ACC:0.86 F1:0.62 REC:0.56 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4244 ACC:0.85 F1:0.71 REC:0.64 PRE:0.78: 100%|| 5/5 [00:10<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.4244 ACC:0.85 F1:0.71 REC:0.64 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3710 LR:0.00000300: 100%|| 34/34 [00:33<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3710 ACC:0.86 F1:0.62 REC:0.56 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5233 ACC:0.77 F1:0.63 REC:0.71 PRE:0.57: 100%|| 5/5 [00:11<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.5233 ACC:0.77 F1:0.63 REC:0.71 PRE:0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3523 LR:0.00000300: 100%|| 34/34 [00:33<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3523 ACC:0.88 F1:0.67 REC:0.60 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4039 ACC:0.86 F1:0.73 REC:0.68 PRE:0.79: 100%|| 5/5 [00:11<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.4039 ACC:0.86 F1:0.73 REC:0.68 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3696 LR:0.00000300: 100%|| 34/34 [00:34<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3696 ACC:0.87 F1:0.63 REC:0.56 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4192 ACC:0.83 F1:0.69 REC:0.68 PRE:0.70: 100%|| 5/5 [00:12<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.4192 ACC:0.83 F1:0.69 REC:0.68 PRE:0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3512 LR:0.00000300: 100%|| 34/34 [00:31<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3512 ACC:0.88 F1:0.67 REC:0.60 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4050 ACC:0.86 F1:0.73 REC:0.68 PRE:0.79: 100%|| 5/5 [00:10<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.4050 ACC:0.86 F1:0.73 REC:0.68 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3456 LR:0.00000300: 100%|| 34/34 [00:29<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3456 ACC:0.89 F1:0.70 REC:0.62 PRE:0.79 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4550 ACC:0.83 F1:0.70 REC:0.71 PRE:0.69: 100%|| 5/5 [00:11<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.4550 ACC:0.83 F1:0.70 REC:0.71 PRE:0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3521 LR:0.00000300: 100%|| 34/34 [00:31<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3521 ACC:0.87 F1:0.65 REC:0.58 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4151 ACC:0.84 F1:0.70 REC:0.68 PRE:0.73: 100%|| 5/5 [00:11<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.4151 ACC:0.84 F1:0.70 REC:0.68 PRE:0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3289 LR:0.00000300: 100%|| 34/34 [00:34<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3289 ACC:0.89 F1:0.71 REC:0.67 PRE:0.77 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4003 ACC:0.85 F1:0.72 REC:0.68 PRE:0.76: 100%|| 5/5 [00:09<00:00,  1.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.4003 ACC:0.85 F1:0.72 REC:0.68 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3277 LR:0.00000300: 100%|| 34/34 [00:28<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3277 ACC:0.89 F1:0.72 REC:0.68 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3889 ACC:0.86 F1:0.73 REC:0.68 PRE:0.79: 100%|| 5/5 [00:11<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.3889 ACC:0.86 F1:0.73 REC:0.68 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3400 LR:0.00000300: 100%|| 34/34 [00:29<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3400 ACC:0.88 F1:0.67 REC:0.58 PRE:0.79 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4931 ACC:0.79 F1:0.66 REC:0.71 PRE:0.61: 100%|| 5/5 [00:10<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.4931 ACC:0.79 F1:0.66 REC:0.71 PRE:0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:11<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana kekeringan\n",
      "Nilai Precision = 0.5777777777777777\n",
      "Nilai Recall = 0.6666666666666666\n",
      "Nilai F1 = 0.619047619047619\n",
      "run ke 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5665 LR:0.00000300: 100%|| 34/34 [00:33<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5665 ACC:0.77 F1:0.06 REC:0.04 PRE:0.20 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5877 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.5877 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5240 LR:0.00000300: 100%|| 34/34 [00:26<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5240 ACC:0.79 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5898 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.5898 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5183 LR:0.00000300: 100%|| 34/34 [00:26<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5183 ACC:0.79 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5899 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:11<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.5899 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5119 LR:0.00000300: 100%|| 34/34 [00:29<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5119 ACC:0.79 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5867 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.5867 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5084 LR:0.00000300: 100%|| 34/34 [00:30<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5084 ACC:0.79 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5825 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:11<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.5825 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4998 LR:0.00000300: 100%|| 34/34 [00:33<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4998 ACC:0.79 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5670 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.5670 ACC:0.72 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4833 LR:0.00000300: 100%|| 34/34 [00:31<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4833 ACC:0.80 F1:0.04 REC:0.02 PRE:1.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5237 ACC:0.73 F1:0.13 REC:0.07 PRE:0.67: 100%|| 5/5 [00:12<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.5237 ACC:0.73 F1:0.13 REC:0.07 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4470 LR:0.00000300: 100%|| 34/34 [00:33<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4470 ACC:0.82 F1:0.35 REC:0.24 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4923 ACC:0.81 F1:0.58 REC:0.46 PRE:0.76: 100%|| 5/5 [00:11<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.4923 ACC:0.81 F1:0.58 REC:0.46 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4539 LR:0.00000300: 100%|| 34/34 [00:33<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4539 ACC:0.83 F1:0.42 REC:0.30 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5323 ACC:0.77 F1:0.65 REC:0.75 PRE:0.57: 100%|| 5/5 [00:10<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.5323 ACC:0.77 F1:0.65 REC:0.75 PRE:0.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4239 LR:0.00000300: 100%|| 34/34 [00:33<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4239 ACC:0.85 F1:0.58 REC:0.52 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4810 ACC:0.81 F1:0.60 REC:0.50 PRE:0.74: 100%|| 5/5 [00:12<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.4810 ACC:0.81 F1:0.60 REC:0.50 PRE:0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4019 LR:0.00000300: 100%|| 34/34 [00:35<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4019 ACC:0.86 F1:0.61 REC:0.55 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4936 ACC:0.80 F1:0.50 REC:0.36 PRE:0.83: 100%|| 5/5 [00:15<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.4936 ACC:0.80 F1:0.50 REC:0.36 PRE:0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4029 LR:0.00000300: 100%|| 34/34 [00:36<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4029 ACC:0.87 F1:0.63 REC:0.55 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5804 ACC:0.72 F1:0.07 REC:0.04 PRE:0.50: 100%|| 5/5 [00:13<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5804 ACC:0.72 F1:0.07 REC:0.04 PRE:0.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4001 LR:0.00000300: 100%|| 34/34 [00:35<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4001 ACC:0.85 F1:0.61 REC:0.56 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5117 ACC:0.79 F1:0.46 REC:0.32 PRE:0.82: 100%|| 5/5 [00:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.5117 ACC:0.79 F1:0.46 REC:0.32 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4024 LR:0.00000300: 100%|| 34/34 [00:31<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4024 ACC:0.85 F1:0.56 REC:0.47 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4768 ACC:0.81 F1:0.68 REC:0.71 PRE:0.65: 100%|| 5/5 [00:10<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.4768 ACC:0.81 F1:0.68 REC:0.71 PRE:0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3837 LR:0.00000300: 100%|| 34/34 [00:25<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3837 ACC:0.87 F1:0.67 REC:0.63 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4558 ACC:0.81 F1:0.67 REC:0.68 PRE:0.66: 100%|| 5/5 [00:11<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.4558 ACC:0.81 F1:0.67 REC:0.68 PRE:0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3778 LR:0.00000300: 100%|| 34/34 [00:19<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3778 ACC:0.86 F1:0.65 REC:0.60 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4526 ACC:0.83 F1:0.64 REC:0.54 PRE:0.79: 100%|| 5/5 [00:06<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.4526 ACC:0.83 F1:0.64 REC:0.54 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3677 LR:0.00000300: 100%|| 34/34 [00:20<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3677 ACC:0.88 F1:0.67 REC:0.61 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4648 ACC:0.81 F1:0.58 REC:0.46 PRE:0.76: 100%|| 5/5 [00:11<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.4648 ACC:0.81 F1:0.58 REC:0.46 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3710 LR:0.00000300: 100%|| 34/34 [00:26<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3710 ACC:0.88 F1:0.66 REC:0.58 PRE:0.77 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4227 ACC:0.84 F1:0.69 REC:0.64 PRE:0.75: 100%|| 5/5 [00:12<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.4227 ACC:0.84 F1:0.69 REC:0.64 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3507 LR:0.00000300: 100%|| 34/34 [00:25<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3507 ACC:0.89 F1:0.70 REC:0.65 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4362 ACC:0.83 F1:0.65 REC:0.57 PRE:0.76: 100%|| 5/5 [00:10<00:00,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.4362 ACC:0.83 F1:0.65 REC:0.57 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3685 LR:0.00000300: 100%|| 34/34 [00:34<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3685 ACC:0.88 F1:0.69 REC:0.67 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4491 ACC:0.82 F1:0.69 REC:0.71 PRE:0.67: 100%|| 5/5 [00:10<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.4491 ACC:0.82 F1:0.69 REC:0.71 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3460 LR:0.00000300: 100%|| 34/34 [00:31<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3460 ACC:0.89 F1:0.70 REC:0.62 PRE:0.80 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4554 ACC:0.83 F1:0.64 REC:0.54 PRE:0.79: 100%|| 5/5 [00:11<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.4554 ACC:0.83 F1:0.64 REC:0.54 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3377 LR:0.00000300: 100%|| 34/34 [00:36<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3377 ACC:0.89 F1:0.73 REC:0.71 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4296 ACC:0.80 F1:0.63 REC:0.61 PRE:0.65: 100%|| 5/5 [00:13<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.4296 ACC:0.80 F1:0.63 REC:0.61 PRE:0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3448 LR:0.00000300: 100%|| 34/34 [00:28<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3448 ACC:0.89 F1:0.71 REC:0.64 PRE:0.80 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4381 ACC:0.84 F1:0.67 REC:0.57 PRE:0.80: 100%|| 5/5 [00:11<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.4381 ACC:0.84 F1:0.67 REC:0.57 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3210 LR:0.00000300: 100%|| 34/34 [00:34<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3210 ACC:0.91 F1:0.76 REC:0.72 PRE:0.80 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4650 ACC:0.83 F1:0.64 REC:0.54 PRE:0.79: 100%|| 5/5 [00:10<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.4650 ACC:0.83 F1:0.64 REC:0.54 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:12<00:00,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana kekeringan\n",
      "Nilai Precision = 0.8571428571428571\n",
      "Nilai Recall = 0.3076923076923077\n",
      "Nilai F1 = 0.45283018867924535\n",
      "Nilai maksimal dari semua run:\n",
      "P = 0.8571428571428571\n",
      "R = 0.6666666666666666\n",
      "F1 = 0.619047619047619\n",
      "Nilai minimal dari semua run:\n",
      "P = 0.5777777777777777\n",
      "R = 0.3076923076923077\n",
      "F1 = 0.45283018867924535\n",
      "Nilai rata-rata dari semua run:\n",
      "P = 0.7174603174603174\n",
      "R = 0.48717948717948717\n",
      "F1 = 0.5359389038634321\n"
     ]
    }
   ],
   "source": [
    "f1_list, p_list, r_list = [],[],[]\n",
    "for i in range(1,3):\n",
    "    print(f\"run ke {i+1}\")\n",
    "    p,r,f = run_bert_for_bencana('kekeringan',i)\n",
    "    f1_list.append(f)\n",
    "    p_list.append(p)\n",
    "    r_list.append(r)\n",
    "display_score_format(f1_list,p_list,r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0bbaa30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ke 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5978 LR:0.00000300: 100%|| 39/39 [00:43<00:00,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5978 ACC:0.73 F1:0.11 REC:0.07 PRE:0.25 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7413 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.7413 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5431 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5431 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7467 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:13<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.7467 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5567 LR:0.00000300: 100%|| 39/39 [00:40<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5567 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7265 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.7265 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5347 LR:0.00000300: 100%|| 39/39 [00:41<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5347 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6462 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6462 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5010 LR:0.00000300: 100%|| 39/39 [00:41<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5010 ACC:0.77 F1:0.06 REC:0.03 PRE:1.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6288 ACC:0.62 F1:0.13 REC:0.07 PRE:1.00: 100%|| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6288 ACC:0.62 F1:0.13 REC:0.07 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4610 LR:0.00000300: 100%|| 39/39 [00:40<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4610 ACC:0.81 F1:0.45 REC:0.33 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4752 ACC:0.83 F1:0.79 REC:0.79 PRE:0.79: 100%|| 5/5 [00:15<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.4752 ACC:0.83 F1:0.79 REC:0.79 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4440 LR:0.00000300: 100%|| 39/39 [00:41<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4440 ACC:0.82 F1:0.57 REC:0.49 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5716 ACC:0.70 F1:0.45 REC:0.30 PRE:0.87: 100%|| 5/5 [00:14<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.5716 ACC:0.70 F1:0.45 REC:0.30 PRE:0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4362 LR:0.00000300: 100%|| 39/39 [00:37<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4362 ACC:0.81 F1:0.53 REC:0.44 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4486 ACC:0.83 F1:0.77 REC:0.72 PRE:0.84: 100%|| 5/5 [00:11<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.4486 ACC:0.83 F1:0.77 REC:0.72 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4261 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4261 ACC:0.83 F1:0.60 REC:0.52 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4330 ACC:0.83 F1:0.77 REC:0.72 PRE:0.84: 100%|| 5/5 [00:12<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.4330 ACC:0.83 F1:0.77 REC:0.72 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4053 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4053 ACC:0.86 F1:0.66 REC:0.59 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4543 ACC:0.83 F1:0.76 REC:0.67 PRE:0.88: 100%|| 5/5 [00:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.4543 ACC:0.83 F1:0.76 REC:0.67 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4069 LR:0.00000300: 100%|| 39/39 [00:40<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4069 ACC:0.84 F1:0.63 REC:0.59 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4449 ACC:0.83 F1:0.76 REC:0.67 PRE:0.88: 100%|| 5/5 [00:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.4449 ACC:0.83 F1:0.76 REC:0.67 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.3877 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.3877 ACC:0.85 F1:0.67 REC:0.62 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7014 ACC:0.65 F1:0.24 REC:0.14 PRE:1.00: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.7014 ACC:0.65 F1:0.24 REC:0.14 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.3999 LR:0.00000300: 100%|| 39/39 [00:36<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.3999 ACC:0.84 F1:0.63 REC:0.57 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5242 ACC:0.74 F1:0.58 REC:0.44 PRE:0.86: 100%|| 5/5 [00:14<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.5242 ACC:0.74 F1:0.58 REC:0.44 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3806 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.3806 ACC:0.85 F1:0.66 REC:0.59 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5144 ACC:0.75 F1:0.61 REC:0.47 PRE:0.87: 100%|| 5/5 [00:12<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5144 ACC:0.75 F1:0.61 REC:0.47 PRE:0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3761 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.3761 ACC:0.86 F1:0.68 REC:0.63 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4162 ACC:0.85 F1:0.79 REC:0.70 PRE:0.91: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.4162 ACC:0.85 F1:0.79 REC:0.70 PRE:0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3657 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.3657 ACC:0.87 F1:0.70 REC:0.65 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4597 ACC:0.82 F1:0.74 REC:0.63 PRE:0.90: 100%|| 5/5 [00:14<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.4597 ACC:0.82 F1:0.74 REC:0.63 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3806 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3806 ACC:0.85 F1:0.66 REC:0.59 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3929 ACC:0.86 F1:0.81 REC:0.72 PRE:0.91: 100%|| 5/5 [00:12<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.3929 ACC:0.86 F1:0.81 REC:0.72 PRE:0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3894 LR:0.00000300: 100%|| 39/39 [00:40<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3894 ACC:0.85 F1:0.64 REC:0.59 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3907 ACC:0.86 F1:0.81 REC:0.72 PRE:0.91: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.3907 ACC:0.86 F1:0.81 REC:0.72 PRE:0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3870 LR:0.00000300: 100%|| 39/39 [00:37<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3870 ACC:0.87 F1:0.70 REC:0.66 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3651 ACC:0.90 F1:0.87 REC:0.88 PRE:0.86: 100%|| 5/5 [00:12<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.3651 ACC:0.90 F1:0.87 REC:0.88 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3661 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3661 ACC:0.86 F1:0.69 REC:0.65 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5014 ACC:0.78 F1:0.67 REC:0.53 PRE:0.88: 100%|| 5/5 [00:13<00:00,  2.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.5014 ACC:0.78 F1:0.67 REC:0.53 PRE:0.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3837 LR:0.00000300: 100%|| 39/39 [00:32<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3837 ACC:0.84 F1:0.62 REC:0.54 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3761 ACC:0.86 F1:0.81 REC:0.77 PRE:0.87: 100%|| 5/5 [00:10<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.3761 ACC:0.86 F1:0.81 REC:0.77 PRE:0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3539 LR:0.00000300: 100%|| 39/39 [00:28<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3539 ACC:0.87 F1:0.68 REC:0.59 PRE:0.79 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3923 ACC:0.84 F1:0.77 REC:0.67 PRE:0.91: 100%|| 5/5 [00:10<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.3923 ACC:0.84 F1:0.77 REC:0.67 PRE:0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3404 LR:0.00000300: 100%|| 39/39 [00:30<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3404 ACC:0.88 F1:0.73 REC:0.70 PRE:0.77 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4601 ACC:0.79 F1:0.69 REC:0.56 PRE:0.89: 100%|| 5/5 [00:15<00:00,  3.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.4601 ACC:0.79 F1:0.69 REC:0.56 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3606 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3606 ACC:0.87 F1:0.71 REC:0.65 PRE:0.79 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4703 ACC:0.79 F1:0.69 REC:0.56 PRE:0.89: 100%|| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.4703 ACC:0.79 F1:0.69 REC:0.56 PRE:0.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:19<00:00,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana longsor\n",
      "Nilai Precision = 0.7096774193548387\n",
      "Nilai Recall = 0.41509433962264153\n",
      "Nilai F1 = 0.5238095238095237\n",
      "run ke 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5637 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5637 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7514 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.7514 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5400 LR:0.00000300: 100%|| 39/39 [00:37<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5400 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7241 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.7241 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5401 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5401 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6943 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:20<00:00,  4.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6943 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5188 LR:0.00000300: 100%|| 39/39 [00:48<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5188 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6622 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:19<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6622 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4995 LR:0.00000300: 100%|| 39/39 [00:49<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.4995 ACC:0.77 F1:0.09 REC:0.05 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5580 ACC:0.68 F1:0.39 REC:0.26 PRE:0.85: 100%|| 5/5 [00:16<00:00,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.5580 ACC:0.68 F1:0.39 REC:0.26 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4654 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4654 ACC:0.80 F1:0.44 REC:0.33 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6574 ACC:0.63 F1:0.17 REC:0.09 PRE:1.00: 100%|| 5/5 [00:13<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.6574 ACC:0.63 F1:0.17 REC:0.09 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4603 LR:0.00000300: 100%|| 39/39 [00:40<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4603 ACC:0.82 F1:0.49 REC:0.37 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5449 ACC:0.72 F1:0.51 REC:0.35 PRE:0.94: 100%|| 5/5 [00:12<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.5449 ACC:0.72 F1:0.51 REC:0.35 PRE:0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4354 LR:0.00000300: 100%|| 39/39 [00:40<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4354 ACC:0.82 F1:0.53 REC:0.44 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6469 ACC:0.68 F1:0.35 REC:0.21 PRE:1.00: 100%|| 5/5 [00:17<00:00,  3.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.6469 ACC:0.68 F1:0.35 REC:0.21 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4463 LR:0.00000300: 100%|| 39/39 [00:29<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4463 ACC:0.82 F1:0.53 REC:0.42 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4761 ACC:0.79 F1:0.71 REC:0.63 PRE:0.82: 100%|| 5/5 [00:06<00:00,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.4761 ACC:0.79 F1:0.71 REC:0.63 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4132 LR:0.00000300: 100%|| 39/39 [00:12<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4132 ACC:0.83 F1:0.59 REC:0.50 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6758 ACC:0.65 F1:0.24 REC:0.14 PRE:1.00: 100%|| 5/5 [00:05<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.6758 ACC:0.65 F1:0.24 REC:0.14 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4077 LR:0.00000300: 100%|| 39/39 [00:25<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4077 ACC:0.84 F1:0.62 REC:0.53 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5869 ACC:0.71 F1:0.46 REC:0.30 PRE:1.00: 100%|| 5/5 [00:06<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.5869 ACC:0.71 F1:0.46 REC:0.30 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4136 LR:0.00000300: 100%|| 39/39 [00:22<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4136 ACC:0.83 F1:0.59 REC:0.51 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5902 ACC:0.72 F1:0.49 REC:0.33 PRE:1.00: 100%|| 5/5 [00:10<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5902 ACC:0.72 F1:0.49 REC:0.33 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4289 LR:0.00000300: 100%|| 39/39 [00:30<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4289 ACC:0.83 F1:0.59 REC:0.50 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4671 ACC:0.80 F1:0.72 REC:0.63 PRE:0.84: 100%|| 5/5 [00:11<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.4671 ACC:0.80 F1:0.72 REC:0.63 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4167 LR:0.00000300: 100%|| 39/39 [00:30<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4167 ACC:0.83 F1:0.61 REC:0.54 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5340 ACC:0.75 F1:0.58 REC:0.42 PRE:0.95: 100%|| 5/5 [00:10<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5340 ACC:0.75 F1:0.58 REC:0.42 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4003 LR:0.00000300: 100%|| 39/39 [00:31<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4003 ACC:0.84 F1:0.61 REC:0.53 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4776 ACC:0.80 F1:0.70 REC:0.56 PRE:0.92: 100%|| 5/5 [00:11<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.4776 ACC:0.80 F1:0.70 REC:0.56 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4128 LR:0.00000300: 100%|| 39/39 [00:47<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4128 ACC:0.84 F1:0.62 REC:0.55 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4655 ACC:0.80 F1:0.70 REC:0.56 PRE:0.92: 100%|| 5/5 [00:14<00:00,  2.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.4655 ACC:0.80 F1:0.70 REC:0.56 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3847 LR:0.00000300: 100%|| 39/39 [00:49<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3847 ACC:0.84 F1:0.63 REC:0.57 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4634 ACC:0.82 F1:0.73 REC:0.60 PRE:0.93: 100%|| 5/5 [00:13<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.4634 ACC:0.82 F1:0.73 REC:0.60 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3868 LR:0.00000300: 100%|| 39/39 [00:27<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3868 ACC:0.86 F1:0.68 REC:0.61 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4828 ACC:0.80 F1:0.70 REC:0.56 PRE:0.92: 100%|| 5/5 [00:06<00:00,  1.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.4828 ACC:0.80 F1:0.70 REC:0.56 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3699 LR:0.00000300: 100%|| 39/39 [00:12<00:00,  3.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3699 ACC:0.86 F1:0.69 REC:0.63 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4600 ACC:0.81 F1:0.72 REC:0.60 PRE:0.90: 100%|| 5/5 [00:06<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.4600 ACC:0.81 F1:0.72 REC:0.60 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3650 LR:0.00000300: 100%|| 39/39 [00:13<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3650 ACC:0.86 F1:0.69 REC:0.65 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4615 ACC:0.81 F1:0.71 REC:0.58 PRE:0.93: 100%|| 5/5 [00:06<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.4615 ACC:0.81 F1:0.71 REC:0.58 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3800 LR:0.00000300: 100%|| 39/39 [00:20<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3800 ACC:0.86 F1:0.66 REC:0.60 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4710 ACC:0.79 F1:0.68 REC:0.53 PRE:0.92: 100%|| 5/5 [00:15<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.4710 ACC:0.79 F1:0.68 REC:0.53 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3761 LR:0.00000300: 100%|| 39/39 [00:39<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3761 ACC:0.85 F1:0.65 REC:0.57 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4558 ACC:0.79 F1:0.68 REC:0.53 PRE:0.92: 100%|| 5/5 [00:12<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.4558 ACC:0.79 F1:0.68 REC:0.53 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3752 LR:0.00000300: 100%|| 39/39 [00:35<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3752 ACC:0.85 F1:0.65 REC:0.57 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4325 ACC:0.82 F1:0.74 REC:0.63 PRE:0.90: 100%|| 5/5 [00:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.4325 ACC:0.82 F1:0.74 REC:0.63 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3584 LR:0.00000300: 100%|| 39/39 [00:48<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3584 ACC:0.87 F1:0.70 REC:0.65 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5210 ACC:0.75 F1:0.59 REC:0.44 PRE:0.90: 100%|| 5/5 [00:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5210 ACC:0.75 F1:0.59 REC:0.44 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:12<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana longsor\n",
      "Nilai Precision = 0.7307692307692307\n",
      "Nilai Recall = 0.3584905660377358\n",
      "Nilai F1 = 0.4810126582278481\n",
      "run ke 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5778 LR:0.00000300: 100%|| 39/39 [00:32<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5778 ACC:0.75 F1:0.03 REC:0.01 PRE:0.20 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7345 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.7345 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5439 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5439 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7335 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.7335 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5499 LR:0.00000300: 100%|| 39/39 [00:41<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5499 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7194 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:08<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.7194 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5425 LR:0.00000300: 100%|| 39/39 [00:30<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5425 ACC:0.76 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6547 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:08<00:00,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6547 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5244 LR:0.00000300: 100%|| 39/39 [00:24<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5244 ACC:0.76 F1:0.01 REC:0.00 PRE:1.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6601 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:10<00:00,  2.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6601 ACC:0.59 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4976 LR:0.00000300: 100%|| 39/39 [00:33<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.4976 ACC:0.77 F1:0.10 REC:0.06 PRE:0.86 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6656 ACC:0.62 F1:0.13 REC:0.07 PRE:1.00: 100%|| 5/5 [00:12<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.6656 ACC:0.62 F1:0.13 REC:0.07 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4697 LR:0.00000300: 100%|| 39/39 [00:46<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.4697 ACC:0.80 F1:0.44 REC:0.33 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5494 ACC:0.71 F1:0.48 REC:0.33 PRE:0.93: 100%|| 5/5 [00:12<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.5494 ACC:0.71 F1:0.48 REC:0.33 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4552 LR:0.00000300: 100%|| 39/39 [00:40<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4552 ACC:0.83 F1:0.55 REC:0.45 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4641 ACC:0.83 F1:0.77 REC:0.72 PRE:0.84: 100%|| 5/5 [00:12<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.4641 ACC:0.83 F1:0.77 REC:0.72 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4477 LR:0.00000300: 100%|| 39/39 [00:32<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4477 ACC:0.82 F1:0.54 REC:0.45 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4478 ACC:0.82 F1:0.77 REC:0.72 PRE:0.82: 100%|| 5/5 [00:12<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.4478 ACC:0.82 F1:0.77 REC:0.72 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4367 LR:0.00000300: 100%|| 39/39 [00:30<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4367 ACC:0.83 F1:0.57 REC:0.48 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4472 ACC:0.82 F1:0.77 REC:0.74 PRE:0.80: 100%|| 5/5 [00:07<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.4472 ACC:0.82 F1:0.77 REC:0.74 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4272 LR:0.00000300: 100%|| 39/39 [00:27<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4272 ACC:0.83 F1:0.61 REC:0.56 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5218 ACC:0.75 F1:0.58 REC:0.42 PRE:0.95: 100%|| 5/5 [00:10<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.5218 ACC:0.75 F1:0.58 REC:0.42 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4204 LR:0.00000300: 100%|| 39/39 [00:37<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4204 ACC:0.85 F1:0.63 REC:0.55 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4771 ACC:0.77 F1:0.66 REC:0.53 PRE:0.85: 100%|| 5/5 [00:12<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.4771 ACC:0.77 F1:0.66 REC:0.53 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4091 LR:0.00000300: 100%|| 39/39 [00:34<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4091 ACC:0.84 F1:0.61 REC:0.53 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4346 ACC:0.83 F1:0.77 REC:0.70 PRE:0.86: 100%|| 5/5 [00:14<00:00,  2.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.4346 ACC:0.83 F1:0.77 REC:0.70 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4157 LR:0.00000300: 100%|| 39/39 [00:38<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4157 ACC:0.86 F1:0.67 REC:0.60 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5513 ACC:0.74 F1:0.56 REC:0.40 PRE:0.94: 100%|| 5/5 [00:11<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5513 ACC:0.74 F1:0.56 REC:0.40 PRE:0.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4255 LR:0.00000300: 100%|| 39/39 [00:41<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4255 ACC:0.83 F1:0.62 REC:0.57 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5963 ACC:0.70 F1:0.46 REC:0.30 PRE:0.93: 100%|| 5/5 [00:13<00:00,  2.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.5963 ACC:0.70 F1:0.46 REC:0.30 PRE:0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4010 LR:0.00000300: 100%|| 39/39 [00:42<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4010 ACC:0.86 F1:0.68 REC:0.63 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4713 ACC:0.80 F1:0.70 REC:0.56 PRE:0.92: 100%|| 5/5 [00:13<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.4713 ACC:0.80 F1:0.70 REC:0.56 PRE:0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3831 LR:0.00000300: 100%|| 39/39 [00:44<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.3831 ACC:0.86 F1:0.67 REC:0.60 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5183 ACC:0.78 F1:0.65 REC:0.49 PRE:0.95: 100%|| 5/5 [00:11<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.5183 ACC:0.78 F1:0.65 REC:0.49 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3966 LR:0.00000300: 100%|| 39/39 [00:36<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.3966 ACC:0.85 F1:0.66 REC:0.61 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5342 ACC:0.76 F1:0.60 REC:0.44 PRE:0.95: 100%|| 5/5 [00:13<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.5342 ACC:0.76 F1:0.60 REC:0.44 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3717 LR:0.00000300: 100%|| 39/39 [00:24<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.3717 ACC:0.86 F1:0.68 REC:0.60 PRE:0.77 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5009 ACC:0.77 F1:0.62 REC:0.47 PRE:0.95: 100%|| 5/5 [00:12<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.5009 ACC:0.77 F1:0.62 REC:0.47 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3740 LR:0.00000300: 100%|| 39/39 [00:41<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3740 ACC:0.87 F1:0.70 REC:0.65 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5194 ACC:0.77 F1:0.62 REC:0.47 PRE:0.95: 100%|| 5/5 [00:13<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.5194 ACC:0.77 F1:0.62 REC:0.47 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3649 LR:0.00000300: 100%|| 39/39 [00:36<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3649 ACC:0.87 F1:0.70 REC:0.63 PRE:0.80 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4235 ACC:0.81 F1:0.73 REC:0.63 PRE:0.87: 100%|| 5/5 [00:12<00:00,  2.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.4235 ACC:0.81 F1:0.73 REC:0.63 PRE:0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3959 LR:0.00000300: 100%|| 39/39 [00:42<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.3959 ACC:0.85 F1:0.66 REC:0.62 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.3943 ACC:0.81 F1:0.74 REC:0.67 PRE:0.83: 100%|| 5/5 [00:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.3943 ACC:0.81 F1:0.74 REC:0.67 PRE:0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3967 LR:0.00000300: 100%|| 39/39 [00:41<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3967 ACC:0.84 F1:0.65 REC:0.62 PRE:0.69 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4711 ACC:0.79 F1:0.67 REC:0.51 PRE:0.96: 100%|| 5/5 [00:15<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.4711 ACC:0.79 F1:0.67 REC:0.51 PRE:0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3596 LR:0.00000300: 100%|| 39/39 [00:42<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.3596 ACC:0.87 F1:0.70 REC:0.64 PRE:0.79 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4948 ACC:0.78 F1:0.65 REC:0.49 PRE:0.95: 100%|| 5/5 [00:13<00:00,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.4948 ACC:0.78 F1:0.65 REC:0.49 PRE:0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:14<00:00,  1.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana longsor\n",
      "Nilai Precision = 0.7105263157894737\n",
      "Nilai Recall = 0.5094339622641509\n",
      "Nilai F1 = 0.5934065934065935\n",
      "Nilai maksimal dari semua run:\n",
      "P = 0.7307692307692307\n",
      "R = 0.5094339622641509\n",
      "F1 = 0.5934065934065935\n",
      "Nilai minimal dari semua run:\n",
      "P = 0.7096774193548387\n",
      "R = 0.3584905660377358\n",
      "F1 = 0.4810126582278481\n",
      "Nilai rata-rata dari semua run:\n",
      "P = 0.7169909886378477\n",
      "R = 0.42767295597484284\n",
      "F1 = 0.5327429251479884\n"
     ]
    }
   ],
   "source": [
    "f1_list, p_list, r_list = [],[],[]\n",
    "for i in range(3):\n",
    "    print(f\"run ke {i+1}\")\n",
    "    p,r,f = run_bert_for_bencana('longsor',i)\n",
    "    f1_list.append(f)\n",
    "    p_list.append(p)\n",
    "    r_list.append(r)\n",
    "display_score_format(f1_list,p_list,r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29ed8f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run ke 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5904 LR:0.00000300: 100%|| 37/37 [00:32<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5904 ACC:0.73 F1:0.01 REC:0.00 PRE:0.33 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6615 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.6615 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5686 LR:0.00000300: 100%|| 37/37 [00:33<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5686 ACC:0.74 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6582 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:11<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.6582 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5887 LR:0.00000300: 100%|| 37/37 [00:36<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5887 ACC:0.74 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6645 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6645 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5533 LR:0.00000300: 100%|| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5533 ACC:0.74 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6412 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6412 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5357 LR:0.00000300: 100%|| 37/37 [00:45<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5357 ACC:0.74 F1:0.03 REC:0.02 PRE:1.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6170 ACC:0.67 F1:0.20 REC:0.11 PRE:1.00: 100%|| 5/5 [00:12<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6170 ACC:0.67 F1:0.20 REC:0.11 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.5112 LR:0.00000300: 100%|| 37/37 [00:43<00:00,  1.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.5112 ACC:0.75 F1:0.23 REC:0.14 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6201 ACC:0.68 F1:0.33 REC:0.22 PRE:0.73: 100%|| 5/5 [00:16<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.6201 ACC:0.68 F1:0.33 REC:0.22 PRE:0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.5064 LR:0.00000300: 100%|| 37/37 [00:47<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.5064 ACC:0.75 F1:0.35 REC:0.25 PRE:0.56 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6182 ACC:0.67 F1:0.35 REC:0.24 PRE:0.64: 100%|| 5/5 [00:18<00:00,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.6182 ACC:0.67 F1:0.35 REC:0.24 PRE:0.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4802 LR:0.00000300: 100%|| 37/37 [00:50<00:00,  1.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4802 ACC:0.79 F1:0.51 REC:0.42 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6753 ACC:0.65 F1:0.10 REC:0.05 PRE:1.00: 100%|| 5/5 [00:16<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.6753 ACC:0.65 F1:0.10 REC:0.05 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4803 LR:0.00000300: 100%|| 37/37 [00:43<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4803 ACC:0.79 F1:0.49 REC:0.38 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6944 ACC:0.59 F1:0.59 REC:0.81 PRE:0.47: 100%|| 5/5 [00:13<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.6944 ACC:0.59 F1:0.59 REC:0.81 PRE:0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.5015 LR:0.00000300: 100%|| 37/37 [00:41<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.5015 ACC:0.75 F1:0.48 REC:0.42 PRE:0.54 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6355 ACC:0.68 F1:0.30 REC:0.19 PRE:0.78: 100%|| 5/5 [00:11<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.6355 ACC:0.68 F1:0.30 REC:0.19 PRE:0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4658 LR:0.00000300: 100%|| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4658 ACC:0.81 F1:0.58 REC:0.49 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6354 ACC:0.68 F1:0.33 REC:0.22 PRE:0.73: 100%|| 5/5 [00:10<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.6354 ACC:0.68 F1:0.33 REC:0.22 PRE:0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4566 LR:0.00000300: 100%|| 37/37 [00:34<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4566 ACC:0.81 F1:0.56 REC:0.45 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6038 ACC:0.74 F1:0.57 REC:0.46 PRE:0.74: 100%|| 5/5 [00:09<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.6038 ACC:0.74 F1:0.57 REC:0.46 PRE:0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4209 LR:0.00000300: 100%|| 37/37 [00:10<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4209 ACC:0.83 F1:0.65 REC:0.57 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6594 ACC:0.68 F1:0.33 REC:0.22 PRE:0.73: 100%|| 5/5 [00:06<00:00,  1.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.6594 ACC:0.68 F1:0.33 REC:0.22 PRE:0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4229 LR:0.00000300: 100%|| 37/37 [00:10<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4229 ACC:0.83 F1:0.64 REC:0.58 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6142 ACC:0.71 F1:0.51 REC:0.41 PRE:0.68: 100%|| 5/5 [00:08<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.6142 ACC:0.71 F1:0.51 REC:0.41 PRE:0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4398 LR:0.00000300: 100%|| 37/37 [00:29<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4398 ACC:0.83 F1:0.65 REC:0.59 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6112 ACC:0.73 F1:0.64 REC:0.65 PRE:0.63: 100%|| 5/5 [00:14<00:00,  2.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.6112 ACC:0.73 F1:0.64 REC:0.65 PRE:0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.5103 LR:0.00000300: 100%|| 37/37 [00:39<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.5103 ACC:0.79 F1:0.59 REC:0.57 PRE:0.60 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6056 ACC:0.71 F1:0.60 REC:0.59 PRE:0.61: 100%|| 5/5 [00:15<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.6056 ACC:0.71 F1:0.60 REC:0.59 PRE:0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.4210 LR:0.00000300: 100%|| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.4210 ACC:0.84 F1:0.64 REC:0.55 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5869 ACC:0.70 F1:0.50 REC:0.41 PRE:0.65: 100%|| 5/5 [00:14<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.5869 ACC:0.70 F1:0.50 REC:0.41 PRE:0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4095 LR:0.00000300: 100%|| 37/37 [00:39<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4095 ACC:0.84 F1:0.66 REC:0.60 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6363 ACC:0.70 F1:0.44 REC:0.32 PRE:0.71: 100%|| 5/5 [00:14<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.6363 ACC:0.70 F1:0.44 REC:0.32 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.4109 LR:0.00000300: 100%|| 37/37 [00:43<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.4109 ACC:0.84 F1:0.66 REC:0.58 PRE:0.77 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5827 ACC:0.75 F1:0.65 REC:0.62 PRE:0.68: 100%|| 5/5 [00:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.5827 ACC:0.75 F1:0.65 REC:0.62 PRE:0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.4109 LR:0.00000300: 100%|| 37/37 [00:42<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.4109 ACC:0.83 F1:0.66 REC:0.60 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5735 ACC:0.75 F1:0.64 REC:0.59 PRE:0.69: 100%|| 5/5 [00:15<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.5735 ACC:0.75 F1:0.64 REC:0.59 PRE:0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3986 LR:0.00000300: 100%|| 37/37 [00:44<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3986 ACC:0.84 F1:0.67 REC:0.60 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6031 ACC:0.71 F1:0.47 REC:0.35 PRE:0.72: 100%|| 5/5 [00:14<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.6031 ACC:0.71 F1:0.47 REC:0.35 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.4145 LR:0.00000300: 100%|| 37/37 [00:47<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.4145 ACC:0.83 F1:0.64 REC:0.58 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5996 ACC:0.71 F1:0.47 REC:0.35 PRE:0.72: 100%|| 5/5 [00:15<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.5996 ACC:0.71 F1:0.47 REC:0.35 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.4054 LR:0.00000300: 100%|| 37/37 [00:44<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.4054 ACC:0.84 F1:0.67 REC:0.60 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6011 ACC:0.70 F1:0.44 REC:0.32 PRE:0.71: 100%|| 5/5 [00:13<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.6011 ACC:0.70 F1:0.44 REC:0.32 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.4014 LR:0.00000300: 100%|| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.4014 ACC:0.84 F1:0.65 REC:0.59 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5642 ACC:0.74 F1:0.57 REC:0.46 PRE:0.74: 100%|| 5/5 [00:15<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5642 ACC:0.74 F1:0.57 REC:0.46 PRE:0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:13<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana tsunami\n",
      "Nilai Precision = 0.5833333333333334\n",
      "Nilai Recall = 0.4375\n",
      "Nilai F1 = 0.5\n",
      "run ke 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.6191 LR:0.00000300: 100%|| 37/37 [00:33<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.6191 ACC:0.73 F1:0.11 REC:0.07 PRE:0.39 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6639 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.6639 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5727 LR:0.00000300: 100%|| 37/37 [00:25<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5727 ACC:0.74 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6555 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:07<00:00,  1.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.6555 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5631 LR:0.00000300: 100%|| 37/37 [00:17<00:00,  2.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5631 ACC:0.74 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6499 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:07<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6499 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5516 LR:0.00000300: 100%|| 37/37 [00:25<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5516 ACC:0.74 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6512 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:11<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6512 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5405 LR:0.00000300: 100%|| 37/37 [00:31<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5405 ACC:0.74 F1:0.01 REC:0.00 PRE:1.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6171 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6171 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.5471 LR:0.00000300: 100%|| 37/37 [00:34<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.5471 ACC:0.76 F1:0.28 REC:0.17 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5756 ACC:0.67 F1:0.46 REC:0.38 PRE:0.58: 100%|| 5/5 [00:12<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.5756 ACC:0.67 F1:0.46 REC:0.38 PRE:0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.5103 LR:0.00000300: 100%|| 37/37 [00:31<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.5103 ACC:0.77 F1:0.40 REC:0.28 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5577 ACC:0.68 F1:0.38 REC:0.27 PRE:0.67: 100%|| 5/5 [00:12<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.5577 ACC:0.68 F1:0.38 REC:0.27 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4826 LR:0.00000300: 100%|| 37/37 [00:29<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4826 ACC:0.79 F1:0.48 REC:0.37 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5370 ACC:0.75 F1:0.59 REC:0.49 PRE:0.75: 100%|| 5/5 [00:11<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.5370 ACC:0.75 F1:0.59 REC:0.49 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4769 LR:0.00000300: 100%|| 37/37 [00:30<00:00,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.4769 ACC:0.79 F1:0.53 REC:0.45 PRE:0.66 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5502 ACC:0.69 F1:0.34 REC:0.22 PRE:0.80: 100%|| 5/5 [00:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.5502 ACC:0.69 F1:0.34 REC:0.22 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4668 LR:0.00000300: 100%|| 37/37 [00:40<00:00,  1.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4668 ACC:0.81 F1:0.57 REC:0.47 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6685 ACC:0.67 F1:0.20 REC:0.11 PRE:1.00: 100%|| 5/5 [00:12<00:00,  2.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.6685 ACC:0.67 F1:0.20 REC:0.11 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.5278 LR:0.00000300: 100%|| 37/37 [00:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.5278 ACC:0.77 F1:0.36 REC:0.24 PRE:0.65 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.7084 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.7084 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4957 LR:0.00000300: 100%|| 37/37 [00:39<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4957 ACC:0.77 F1:0.43 REC:0.32 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5254 ACC:0.77 F1:0.69 REC:0.70 PRE:0.68: 100%|| 5/5 [00:14<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5254 ACC:0.77 F1:0.69 REC:0.70 PRE:0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4464 LR:0.00000300: 100%|| 37/37 [00:39<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4464 ACC:0.82 F1:0.60 REC:0.51 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5070 ACC:0.77 F1:0.68 REC:0.65 PRE:0.71: 100%|| 5/5 [00:15<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.5070 ACC:0.77 F1:0.68 REC:0.65 PRE:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4534 LR:0.00000300: 100%|| 37/37 [00:38<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4534 ACC:0.81 F1:0.57 REC:0.48 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5005 ACC:0.75 F1:0.56 REC:0.43 PRE:0.80: 100%|| 5/5 [00:11<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5005 ACC:0.75 F1:0.56 REC:0.43 PRE:0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4287 LR:0.00000300: 100%|| 37/37 [00:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4287 ACC:0.82 F1:0.63 REC:0.56 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5573 ACC:0.73 F1:0.47 REC:0.32 PRE:0.86: 100%|| 5/5 [00:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.5573 ACC:0.73 F1:0.47 REC:0.32 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4341 LR:0.00000300: 100%|| 37/37 [00:42<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4341 ACC:0.83 F1:0.62 REC:0.52 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6109 ACC:0.70 F1:0.38 REC:0.24 PRE:0.82: 100%|| 5/5 [00:15<00:00,  3.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.6109 ACC:0.70 F1:0.38 REC:0.24 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.4266 LR:0.00000300: 100%|| 37/37 [00:41<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.4266 ACC:0.83 F1:0.63 REC:0.54 PRE:0.76 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5039 ACC:0.83 F1:0.77 REC:0.78 PRE:0.76: 100%|| 5/5 [00:15<00:00,  3.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.5039 ACC:0.83 F1:0.77 REC:0.78 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4290 LR:0.00000300: 100%|| 37/37 [00:43<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4290 ACC:0.82 F1:0.62 REC:0.55 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5525 ACC:0.71 F1:0.43 REC:0.30 PRE:0.79: 100%|| 5/5 [00:12<00:00,  2.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.5525 ACC:0.71 F1:0.43 REC:0.30 PRE:0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.4249 LR:0.00000300: 100%|| 37/37 [00:29<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.4249 ACC:0.82 F1:0.62 REC:0.56 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6913 ACC:0.65 F1:0.10 REC:0.05 PRE:1.00: 100%|| 5/5 [00:11<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.6913 ACC:0.65 F1:0.10 REC:0.05 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.4751 LR:0.00000300: 100%|| 37/37 [00:31<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.4751 ACC:0.82 F1:0.58 REC:0.47 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5156 ACC:0.73 F1:0.49 REC:0.35 PRE:0.81: 100%|| 5/5 [00:13<00:00,  2.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.5156 ACC:0.73 F1:0.49 REC:0.35 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.4457 LR:0.00000300: 100%|| 37/37 [00:41<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.4457 ACC:0.85 F1:0.68 REC:0.62 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4490 ACC:0.80 F1:0.68 REC:0.57 PRE:0.84: 100%|| 5/5 [00:15<00:00,  3.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.4490 ACC:0.80 F1:0.68 REC:0.57 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.4013 LR:0.00000300: 100%|| 37/37 [00:40<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.4013 ACC:0.85 F1:0.70 REC:0.64 PRE:0.77 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4422 ACC:0.82 F1:0.74 REC:0.68 PRE:0.81: 100%|| 5/5 [00:16<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.4422 ACC:0.82 F1:0.74 REC:0.68 PRE:0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.4038 LR:0.00000300: 100%|| 37/37 [00:39<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.4038 ACC:0.85 F1:0.69 REC:0.62 PRE:0.78 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6517 ACC:0.66 F1:0.66 REC:0.89 PRE:0.52: 100%|| 5/5 [00:14<00:00,  2.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.6517 ACC:0.66 F1:0.66 REC:0.89 PRE:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.5739 LR:0.00000300: 100%|| 37/37 [00:38<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.5739 ACC:0.71 F1:0.41 REC:0.38 PRE:0.45 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5945 ACC:0.65 F1:0.64 REC:0.84 PRE:0.52: 100%|| 5/5 [00:15<00:00,  3.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5945 ACC:0.65 F1:0.64 REC:0.84 PRE:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:15<00:00,  1.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana tsunami\n",
      "Nilai Precision = 0.44086021505376344\n",
      "Nilai Recall = 0.8541666666666666\n",
      "Nilai F1 = 0.5815602836879433\n",
      "run ke 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-large-p2 and are newly initialized because the shapes did not match:\n",
      "- embeddings.word_embeddings.weight: found shape torch.Size([30522, 1024]) in the checkpoint and torch.Size([30522, 128]) in the model instantiated\n",
      "- embeddings.position_embeddings.weight: found shape torch.Size([512, 1024]) in the checkpoint and torch.Size([512, 128]) in the model instantiated\n",
      "- embeddings.token_type_embeddings.weight: found shape torch.Size([2, 1024]) in the checkpoint and torch.Size([2, 128]) in the model instantiated\n",
      "- embeddings.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- embeddings.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.0.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.0.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.1.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.1.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.2.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.2.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.3.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.3.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.4.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.4.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.5.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.5.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.6.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.6.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.7.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.7.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.8.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.8.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.9.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.9.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.10.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.10.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.11.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.11.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.12.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.12.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.13.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.13.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.14.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.14.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.15.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.15.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.16.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.16.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.17.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.17.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.18.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.18.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.19.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.19.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.20.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.20.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.21.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.21.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.22.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.22.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.query.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.key.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.self.value.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.attention.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.intermediate.dense.weight: found shape torch.Size([4096, 1024]) in the checkpoint and torch.Size([4096, 128]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.weight: found shape torch.Size([1024, 4096]) in the checkpoint and torch.Size([128, 4096]) in the model instantiated\n",
      "- encoder.layer.23.output.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.weight: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- encoder.layer.23.output.LayerNorm.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "- pooler.dense.weight: found shape torch.Size([1024, 1024]) in the checkpoint and torch.Size([128, 128]) in the model instantiated\n",
      "- pooler.dense.bias: found shape torch.Size([1024]) in the checkpoint and torch.Size([128]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5966 LR:0.00000300: 100%|| 37/37 [00:46<00:00,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.5966 ACC:0.71 F1:0.05 REC:0.03 PRE:0.18 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6622 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:15<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS:0.6622 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5716 LR:0.00000300: 100%|| 37/37 [00:38<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.5716 ACC:0.74 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6683 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:11<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS:0.6683 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5900 LR:0.00000300: 100%|| 37/37 [00:32<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.5900 ACC:0.74 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6442 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:12<00:00,  2.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS:0.6442 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5574 LR:0.00000300: 100%|| 37/37 [00:31<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.5574 ACC:0.74 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6571 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS:0.6571 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5369 LR:0.00000300: 100%|| 37/37 [00:38<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.5369 ACC:0.73 F1:0.00 REC:0.00 PRE:0.00 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6217 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00: 100%|| 5/5 [00:14<00:00,  2.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS:0.6217 ACC:0.63 F1:0.00 REC:0.00 PRE:0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.5235 LR:0.00000300: 100%|| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.5235 ACC:0.76 F1:0.21 REC:0.12 PRE:0.78 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6014 ACC:0.65 F1:0.56 REC:0.59 PRE:0.52: 100%|| 5/5 [00:13<00:00,  2.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS:0.6014 ACC:0.65 F1:0.56 REC:0.59 PRE:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.5252 LR:0.00000300: 100%|| 37/37 [00:39<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.5252 ACC:0.76 F1:0.43 REC:0.34 PRE:0.58 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6053 ACC:0.69 F1:0.28 REC:0.16 PRE:1.00: 100%|| 5/5 [00:14<00:00,  2.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS:0.6053 ACC:0.69 F1:0.28 REC:0.16 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4984 LR:0.00000300: 100%|| 37/37 [00:37<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.4984 ACC:0.77 F1:0.38 REC:0.27 PRE:0.64 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5632 ACC:0.71 F1:0.38 REC:0.24 PRE:0.90: 100%|| 5/5 [00:13<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS:0.5632 ACC:0.71 F1:0.38 REC:0.24 PRE:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.5154 LR:0.00000300: 100%|| 37/37 [00:40<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.5154 ACC:0.80 F1:0.52 REC:0.41 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.6104 ACC:0.70 F1:0.32 REC:0.19 PRE:1.00: 100%|| 5/5 [00:17<00:00,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS:0.6104 ACC:0.70 F1:0.32 REC:0.19 PRE:1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4678 LR:0.00000300: 100%|| 37/37 [00:46<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.4678 ACC:0.80 F1:0.54 REC:0.45 PRE:0.67 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5361 ACC:0.72 F1:0.50 REC:0.38 PRE:0.74: 100%|| 5/5 [00:16<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS:0.5361 ACC:0.72 F1:0.50 REC:0.38 PRE:0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4772 LR:0.00000300: 100%|| 37/37 [00:43<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.4772 ACC:0.81 F1:0.59 REC:0.52 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5232 ACC:0.75 F1:0.59 REC:0.49 PRE:0.75: 100%|| 5/5 [00:15<00:00,  3.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) VALID LOSS:0.5232 ACC:0.75 F1:0.59 REC:0.49 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4726 LR:0.00000300: 100%|| 37/37 [00:38<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.4726 ACC:0.80 F1:0.50 REC:0.36 PRE:0.78 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5245 ACC:0.77 F1:0.65 REC:0.57 PRE:0.75: 100%|| 5/5 [00:10<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) VALID LOSS:0.5245 ACC:0.77 F1:0.65 REC:0.57 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4402 LR:0.00000300: 100%|| 37/37 [00:33<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.4402 ACC:0.82 F1:0.65 REC:0.61 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5133 ACC:0.75 F1:0.59 REC:0.49 PRE:0.75: 100%|| 5/5 [00:06<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) VALID LOSS:0.5133 ACC:0.75 F1:0.59 REC:0.49 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4580 LR:0.00000300: 100%|| 37/37 [00:23<00:00,  1.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.4580 ACC:0.81 F1:0.59 REC:0.51 PRE:0.70 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5170 ACC:0.76 F1:0.61 REC:0.51 PRE:0.76: 100%|| 5/5 [00:11<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) VALID LOSS:0.5170 ACC:0.76 F1:0.61 REC:0.51 PRE:0.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4640 LR:0.00000300: 100%|| 37/37 [00:31<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.4640 ACC:0.80 F1:0.54 REC:0.45 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5202 ACC:0.76 F1:0.64 REC:0.57 PRE:0.72: 100%|| 5/5 [00:11<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) VALID LOSS:0.5202 ACC:0.76 F1:0.64 REC:0.57 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4306 LR:0.00000300: 100%|| 37/37 [00:31<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.4306 ACC:0.82 F1:0.62 REC:0.53 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5144 ACC:0.76 F1:0.62 REC:0.54 PRE:0.74: 100%|| 5/5 [00:13<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) VALID LOSS:0.5144 ACC:0.76 F1:0.62 REC:0.54 PRE:0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.4115 LR:0.00000300: 100%|| 37/37 [00:34<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.4115 ACC:0.83 F1:0.64 REC:0.59 PRE:0.71 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5289 ACC:0.77 F1:0.67 REC:0.62 PRE:0.72: 100%|| 5/5 [00:17<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) VALID LOSS:0.5289 ACC:0.77 F1:0.67 REC:0.62 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4110 LR:0.00000300: 100%|| 37/37 [00:45<00:00,  1.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.4110 ACC:0.84 F1:0.67 REC:0.61 PRE:0.73 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5685 ACC:0.76 F1:0.68 REC:0.70 PRE:0.67: 100%|| 5/5 [00:13<00:00,  2.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) VALID LOSS:0.5685 ACC:0.76 F1:0.68 REC:0.70 PRE:0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.4075 LR:0.00000300: 100%|| 37/37 [00:42<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.4075 ACC:0.83 F1:0.66 REC:0.60 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5098 ACC:0.75 F1:0.59 REC:0.49 PRE:0.75: 100%|| 5/5 [00:16<00:00,  3.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) VALID LOSS:0.5098 ACC:0.75 F1:0.59 REC:0.49 PRE:0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3947 LR:0.00000300: 100%|| 37/37 [00:36<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.3947 ACC:0.85 F1:0.69 REC:0.64 PRE:0.74 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5354 ACC:0.77 F1:0.67 REC:0.62 PRE:0.72: 100%|| 5/5 [00:13<00:00,  2.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) VALID LOSS:0.5354 ACC:0.77 F1:0.67 REC:0.62 PRE:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3975 LR:0.00000300: 100%|| 37/37 [00:35<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.3975 ACC:0.85 F1:0.70 REC:0.66 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5210 ACC:0.77 F1:0.66 REC:0.59 PRE:0.73: 100%|| 5/5 [00:11<00:00,  2.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) VALID LOSS:0.5210 ACC:0.77 F1:0.66 REC:0.59 PRE:0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.4368 LR:0.00000300: 100%|| 37/37 [00:29<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.4368 ACC:0.81 F1:0.59 REC:0.52 PRE:0.68 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5644 ACC:0.75 F1:0.68 REC:0.70 PRE:0.65: 100%|| 5/5 [00:11<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) VALID LOSS:0.5644 ACC:0.75 F1:0.68 REC:0.70 PRE:0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3873 LR:0.00000300: 100%|| 37/37 [00:30<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.3873 ACC:0.85 F1:0.70 REC:0.68 PRE:0.72 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.4980 ACC:0.77 F1:0.63 REC:0.54 PRE:0.77: 100%|| 5/5 [00:10<00:00,  2.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) VALID LOSS:0.4980 ACC:0.77 F1:0.63 REC:0.54 PRE:0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.4168 LR:0.00000300: 100%|| 37/37 [00:35<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.4168 ACC:0.84 F1:0.68 REC:0.62 PRE:0.75 LR:0.00000300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.5630 ACC:0.75 F1:0.68 REC:0.70 PRE:0.65: 100%|| 5/5 [00:15<00:00,  3.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) VALID LOSS:0.5630 ACC:0.75 F1:0.68 REC:0.70 PRE:0.65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 9/9 [00:15<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluasi bencana tsunami\n",
      "Nilai Precision = 0.5070422535211268\n",
      "Nilai Recall = 0.75\n",
      "Nilai F1 = 0.6050420168067226\n",
      "Nilai maksimal dari semua run:\n",
      "P = 0.5833333333333334\n",
      "R = 0.8541666666666666\n",
      "F1 = 0.6050420168067226\n",
      "Nilai minimal dari semua run:\n",
      "P = 0.44086021505376344\n",
      "R = 0.4375\n",
      "F1 = 0.5\n",
      "Nilai rata-rata dari semua run:\n",
      "P = 0.5104119339694079\n",
      "R = 0.6805555555555555\n",
      "F1 = 0.5622007668315554\n"
     ]
    }
   ],
   "source": [
    "f1_list, p_list, r_list = [],[],[]\n",
    "for i in range(3):\n",
    "    print(f\"run ke {i+1}\")\n",
    "    p,r,f = run_bert_for_bencana('tsunami',i)\n",
    "    f1_list.append(f)\n",
    "    p_list.append(p)\n",
    "    r_list.append(r)\n",
    "display_score_format(f1_list,p_list,r_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb40044a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
